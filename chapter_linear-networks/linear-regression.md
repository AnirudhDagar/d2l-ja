# 線形回帰
:label:`sec_linear_regression`

*回帰* とは、モデリングのための一連の手法のことです。
1 つまたは複数の独立変数と従属変数の関係。自然科学と社会科学では、回帰の目的は最も頻繁に
*入力と出力の関係をキャラクタライズ* します。
一方、機械学習は*予測*に関係することが最も多いです。 

数値を予測したいときはいつでも回帰問題が現れます。一般的な例としては、（住宅、株などの）価格の予測、滞在期間の予測（入院患者の場合）、需要予測（小売売上高）などがあります。すべての予測問題が古典的な回帰問題というわけではありません。以降のセクションでは、分類問題を紹介します。分類問題は、一連のカテゴリ間のメンバシップを予測することを目的としています。 

## 線形回帰の基本要素

*線形回帰*はどちらも最も単純な場合があります
回帰の標準的なツールの中で最も人気があります。デート 19世紀の夜明けに戻る, 線形回帰はいくつかの単純な仮定から流れます.まず、独立変数 $\mathbf{x}$ と従属変数 $y$ の関係は線形であると仮定します。つまり、$y$ は $\mathbf{x}$ の要素の加重和として表すことができます。次に、ノイズはすべて (ガウス分布に従って) 適切に動作すると仮定します。 

アプローチのモチベーションを高めるために、実行例から始めましょう。面積 (平方フィート) と年齢 (年数) に基づいて住宅価格 (ドル) を見積もるとします。住宅価格を予測するモデルを実際に開発するには、各住宅の販売価格、面積、年齢がわかっている売上高で構成されるデータセットを手に入れる必要があります。機械学習の用語では、データセットは*トレーニングデータセット* または*トレーニングセット* と呼ばれ、各行（ここでは 1 つの売上に対応するデータ）は*example*（または*データポイント*、*data instance*、*sample*）と呼ばれます。私たちが予測しようとしているもの（価格）を*ラベル*（または*ターゲット*）と呼びます。予測の基になる独立変数 (年齢と面積) は、*特徴* (または*共変量*) と呼ばれます。 

通常、$n$ を使用して、データセット内のサンプル数を示します。データ例を $i$ で索引付けし、各入力を $\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top$、対応するラベルを $y^{(i)}$ と示します。 

### 線形モデル
:label:`subsec_linear_model`

直線性の仮定では、ターゲット（価格）はフィーチャ（面積と年齢）の加重和として表すことができるとだけ言っています。 

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.$$
:eqlabel:`eq_price-area`

:eqref:`eq_price-area` では $w_{\mathrm{area}}$ と $w_{\mathrm{age}}$ は*ウェイト* と呼ばれ、$b$ は*バイアス* (*オフセット* または*インターセプト* とも呼ばれる) と呼ばれています。重みは各特徴が予測に及ぼす影響を決定し、バイアスはすべての特徴が値0になったときに予測価格が取るべき価値を示すだけです。面積がゼロの家、または正確に0年前の家を見ることは決してない場合でも、偏見が必要です。そうしないと、モデルの表現力が制限されます。厳密に言うと、:eqref:`eq_price-area` は入力特徴量の*アフィン変換* であり、加重和による特徴の*線形変換* と、追加されたバイアスによる*平行移動*の組み合わせによって特徴付けられます。 

データセットが与えられた場合、私たちの目標は、モデルに従って行われた予測がデータで観測された真の価格に最もよく適合するように、重み $\mathbf{w}$ とバイアス $b$ を選択することです。入力フィーチャのアフィン変換によって出力予測が決定されるモデルは*線形モデル* で、アフィン変換は選択した重みとバイアスによって指定されます。 

特徴量が少ないデータセットに注目するのが一般的な分野では、このように長い形式のモデルを明示的に表現するのが一般的です。機械学習では通常、高次元のデータセットを扱うため、線形代数表記法を採用した方が便利です。入力が $d$ の特徴量で構成されている場合、予測 $\hat{y}$ (一般に「帽子」記号は推定値を表します) を次のように表します。 

$$\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$

すべての特徴をベクトル $\mathbf{x} \in \mathbb{R}^d$ に、すべての重みをベクトル $\mathbf{w} \in \mathbb{R}^d$ にまとめると、ドット積を使用してモデルをコンパクトに表現できます。 

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
:eqlabel:`eq_linreg-y`

:eqref:`eq_linreg-y` では、ベクトル $\mathbf{x}$ は 1 つのデータ例の特徴量に対応しています。$n$ の例のデータセット全体の特徴を、*設計行列* $\mathbf{X} \in \mathbb{R}^{n \times d}$ で参照すると便利なことがよくあります。ここで $\mathbf{X}$ には、例ごとに 1 つの行、フィーチャごとに 1 つの列が含まれています。 

特徴の集合 $\mathbf{X}$ の場合、予測値 $\hat{\mathbf{y}} \in \mathbb{R}^n$ は行列とベクトルの積で表すことができます。 

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

ここで、加算中にブロードキャスト (:numref:`subsec_broadcasting` を参照) が適用されます。トレーニングデータセット $\mathbf{X}$ と対応する (既知の) ラベル $\mathbf{y}$ の特性を考えると、線形回帰の目標は、$\mathbf{X}$ と同じ分布からサンプリングされた新しいデータ例の特性に基づいて、重みベクトル $\mathbf{w}$ とバイアス項 $b$ を求めることです。新しい例のラベルは (expectation) は、最小の誤差で予測されます。 

$\mathbf{x}$ が与えられた $y$ を予測するための最良のモデルが線形であると信じたとしても、$1 \leq i \leq n$ すべてについて $y^{(i)}$ が $\mathbf{w}^\top \mathbf{x}^{(i)}+b$ と正確に等しい $n$ の実世界のデータセットを見つけることは期待できません。たとえば、フィーチャ $\mathbf{X}$ および $\mathbf{y}$ の観測に使用する計測器には、わずかな測定誤差が生じる可能性があります。したがって、根底にある関係が線形であると確信している場合でも、そのような誤差を説明するためにノイズ項を取り入れます。 

最適な*パラメータ* (または*モデルパラメータ*) $\mathbf{w}$ と $b$ を検索する前に、(i) 特定のモデルの品質測定と、(ii) 品質を向上させるためにモデルを更新する手順の 2 つが必要です。 

### 損失関数

データをモデルに「あてはめる」方法を考える前に、*適合性*の尺度を決定する必要があります。*loss 関数* は、ターゲットの*実数*と*予測*値の間の距離を定量化します。通常、損失は負ではない数値で、値が小さいほど良好で、完全な予測では損失が0になります。回帰問題で最も一般的な損失関数は二乗誤差です。例 $i$ の予測が $\hat{y}^{(i)}$ で、対応する真のラベルが $y^{(i)}$ の場合、二乗誤差は次の式で求められます。 

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
:eqlabel:`eq_mse`

定数 $\frac{1}{2}$ は実質的な違いはありませんが、表記上は便利で、損失の導関数を取ると相殺されます。トレーニングデータセットは私たちに与えられ、制御不能であるため、経験的誤差はモデルパラメータの関数にすぎません。より具体的にするために、:numref:`fig_fit_linreg` に示すように、1 次元のケースに対する回帰問題をプロットする以下の例を考えてみましょう。 

![Fit data with a linear model.](../img/fit-linreg.svg)
:label:`fig_fit_linreg`

推定値 $\hat{y}^{(i)}$ と観測値 $y^{(i)}$ の間に大きな差があると、二次依存性のため、損失への寄与がさらに大きくなることに注意してください。$n$ の例のデータセット全体でモデルの品質を測定するには、トレーニングセットの損失を単純に平均 (または同等に合計) します。 

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

モデルに学習をさせる場合、すべての学習例で総損失を最小化するパラメーター ($\mathbf{w}^*, b^*$) を求めます。 

$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$

### 分析的ソリューション

線形回帰はたまたま非常に単純な最適化問題です。本書で取り上げている他のほとんどのモデルとは異なり、線形回帰は簡単な公式を適用することで解析的に解くことができます。まず、すべて 1 で構成される計画行列に列を追加することで、バイアス $b$ をパラメーター $\mathbf{w}$ に含めることができます。次に、予測問題は $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$ を最小化することです。損失曲面には1つの臨界点しかなく、ドメイン全体の損失の最小値に相当します。$\mathbf{w}$ に対する損失の導関数をゼロに設定すると、解析的 (閉形式) 解が得られます。 

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.$$

線形回帰のような単純な問題は解析的な解を認めるかもしれないが、そのような幸運に慣れるべきではない。解析的解では優れた数学的解析が可能ですが、解析解の要件は非常に厳しく、ディープラーニングのすべてが除外されます。 

### ミニバッチ確率的勾配降下法

モデルを解析的に解くことができない場合でも、実際にモデルを効果的にトレーニングできることが分かります。さらに、多くのタスクでは、最適化が難しいモデルの方がはるかに優れているため、それらをトレーニングする方法を理解することは、トラブルに見合うだけの価値があります。 

ほぼすべてのディープラーニングモデルを最適化するための重要な手法は、損失関数を徐々に低下させる方向にパラメーターを更新することで、エラーを繰り返し減らすことです。このアルゴリズムを*勾配降下* と呼びます。 

勾配降下法を最も単純に適用するには、損失関数の導関数を使用します。損失関数は、データセット内の各例で計算された損失の平均値です。実際には、この処理は非常に遅くなる可能性があります。1 回の更新を行う前に、データセット全体を渡す必要があります。したがって、更新を計算する必要があるたびに、サンプルのランダムなミニバッチをサンプリングすることに決まることがよくあります。これは、*minibatch 確率的勾配降下* と呼ばれるバリアントです。 

各反復で、まず、一定数の学習例で構成されるミニバッチ $\mathcal{B}$ をランダムにサンプリングします。次に、モデルパラメーターに関して、ミニバッチの平均損失の微分 (勾配) を計算します。最後に、勾配に所定の正の値 $\eta$ を掛け、その結果の項を現在のパラメーター値から減算します。 

更新は次のように数学的に表現できます ($\partial$ は偏微分を表します)。 

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

要約すると、アルゴリズムのステップは次のとおりです。(i) 通常はランダムにモデルパラメーターの値を初期化します。(ii) データからランダムなミニバッチを繰り返しサンプリングし、負の勾配の方向にパラメーターを更新します。二次損失とアフィン変換の場合、これを次のように明示的に記述できます。 

$$\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}$$
:eqlabel:`eq_linreg_batch_update`

$\mathbf{w}$ と $\mathbf{x}$ は :eqref:`eq_linreg_batch_update` のベクトルであることに注意してください。ここでは、より洗練されたベクトル表記法により、係数 ($w_1, w_2, \ldots, w_d$) で物事を表現するよりも数学がはるかに読みやすくなります。設定されたカーディナリティ $|\mathcal{B}|$ は各ミニバッチの例数 (*バッチサイズ*) を表し、$\eta$ は*学習率* を表します。バッチサイズと学習率の値は手作業であらかじめ指定されており、通常はモデルトレーニングでは学習されないことを強調します。調整可能だが学習ループでは更新されないこれらのパラメーターは、*hyperparameters* と呼ばれます。
*ハイパーパラメータチューニング* は、ハイパーパラメータを選択するプロセスです。
通常、個別の*validationデータセット* (または*validationset*) で評価されたトレーニングループの結果に基づいて調整する必要があります。 

あらかじめ決められた反復回数のトレーニングの後 (または他の停止基準が満たされるまで)、推定されたモデルパラメーター ($\hat{\mathbf{w}}, \hat{b}$) を記録します。関数が真に線形でノイズがない場合でも、これらのパラメーターは損失の正確な最小化にはならないことに注意してください。アルゴリズムは最小化器に向かってゆっくりと収束しますが、有限ステップ数では正確に収束できないためです。 

線形回帰は、ドメイン全体で最小値が1つしかない学習問題です。ただし、ディープネットワークのようなより複雑なモデルでは、損失曲面には多くの最小値が含まれます。幸いなことに、まだ完全には理解されていない理由から、ディープラーニングの実践者は、*トレーニングセット*の損失を最小限に抑えるパラメーターを見つけるのに苦労することはほとんどありません。より手ごわい作業は、これまでに見たことのないデータの損失を低く抑えるパラメータを見つけることです。これは*一般化*と呼ばれる課題です。本全体を通して、これらのトピックに戻ります。 

### 学習したモデルで予測を行う

学習済みの線形回帰モデル $\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$ を考えると、面積 $x_1$、年齢 $x_2$ から新しい家 (トレーニングデータに含まれていない) の価格を見積もることができます。特徴量からターゲットを推定することは、一般に*予測* または*推論* と呼ばれます。 

ディープラーニングでは標準的な専門用語として浮上しているにもかかわらず、このステップを「推論」と呼ぶのはやや誤称なので、私たちは*予測*に固執しようとします。統計学では、*推論* はデータセットに基づくパラメーターの推定を意味することが多いです。このような用語の誤用は、ディープラーニングの実践者が統計学者と話をするときによくある混乱の原因となります。 

## 高速化のためのベクタ変換

モデルをトレーニングする場合、通常、サンプルのミニバッチ全体を同時に処理します。これを効率的に行うには (**we**) (~~should~~) (**計算をベクトル化し、Pythonでコストがかかる for ループを書くのではなく、高速な線形代数ライブラリを活用する**) が必要です。

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np
import time
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import numpy as np
import time
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
import numpy as np
import time
```

なぜこれが重要なのかを説明するために、(**ベクトルを加えるための2つの方法を考えてみる**) ことができます。まず、すべて 1 を含む 10000 次元のベクトルを 2 つインスタンス化します。ある方法では、Python の for ループでベクトルをループします。もう 1 つの方法では、`+` を 1 回呼び出すだけで済みます。

```{.python .input}
#@tab all
n = 10000
a = d2l.ones(n)
b = d2l.ones(n)
```

本書では頻繁に実行時間のベンチマークを行いますので、[**タイマーを定義しましょう**]。

```{.python .input}
#@tab all
class Timer:  #@save
    """Record multiple running times."""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """Start the timer."""
        self.tik = time.time()

    def stop(self):
        """Stop the timer and record the time in a list."""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """Return the average time."""
        return sum(self.times) / len(self.times)

    def sum(self):
        """Return the sum of time."""
        return sum(self.times)

    def cumsum(self):
        """Return the accumulated time."""
        return np.array(self.times).cumsum().tolist()
```

これで、ワークロードのベンチマークが可能になりました。まず、[**for-loopを使って座標を1つずつ加算します**]

```{.python .input}
#@tab mxnet, pytorch
c = d2l.zeros(n)
timer = Timer()
for i in range(n):
    c[i] = a[i] + b[i]
f'{timer.stop():.5f} sec'
```

```{.python .input}
#@tab tensorflow
c = tf.Variable(d2l.zeros(n))
timer = Timer()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'{timer.stop():.5f} sec'
```

(**または、再ロードされた `+` 演算子を使用して要素単位の合計を計算します。**)

```{.python .input}
#@tab all
timer.start()
d = a + b
f'{timer.stop():.5f} sec'
```

2番目の方法は最初の方法よりも劇的に高速であることに気付いたでしょう。コードをベクトル化すると、多くの場合、桁違いに高速化されます。さらに、数学をより多くライブラリにプッシュし、自分で計算を記述する必要がないため、エラーの可能性を減らすことができます。 

## 正規分布と二乗損失
:label:`subsec_normal_distribution_and_squared_loss`

上記の情報だけを使ってもすでに手を汚すことはできますが、以下ではノイズの分布に関する仮定を通して、二乗損失目標をより正式に動機づけることができます。 

線形回帰は1795年にGaussによって発明され、Gaussも正規分布（*Gaussian*とも呼ばれる）を発見しました。正規分布と線形回帰の関係は、一般的な親子関係よりも深いことが分かります。記憶を更新するために、平均 $\mu$、分散 $\sigma^2$ (標準偏差 $\sigma$) をもつ正規分布の確率密度は次のように与えられます。 

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

以下 [**正規分布を計算するPython関数を定義します**]。

```{.python .input}
#@tab all
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

これで (**正規分布を可視化**) できます。

```{.python .input}
#@tab all
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

ご覧のとおり、平均値を変更すると $x$ 軸に沿ったシフトに相当し、分散を大きくすると分布が広がり、ピークが小さくなります。 

平均二乗誤差損失関数 (または単に二乗損失) を使用して線形回帰を動機付ける方法の 1 つは、観測値がノイズの多い観測値から発生すると公式に仮定することです。この場合、ノイズは次のように正規分布します。 

$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).$$

したがって、指定された $\mathbf{x}$ の特定の $y$ が見られる*可能性*を 

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$

最尤法の原則によると、パラメーター $\mathbf{w}$ と $b$ の最良値は、データセット全体の「尤度」を最大化する値です。 

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$

最尤法の原理に従って選択された推定量は、*最尤推定量* と呼ばれます。多くの指数関数の積を最大化するのは難しいように思えるかもしれませんが、その代わりに尤度の対数を最大化することで、目的を変えずに物事を大幅に単純化することができます。歴史的な理由から、最適化は最大化ではなく最小化として表現されることが多いです。したがって、何も変更せずに、*負の対数尤度* $-\log P(\mathbf y \mid \mathbf X)$を最小化できます。数学を考えると次のことが得られます。 

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$

ここで、$\sigma$が固定定数であるという仮定をもう1つだけ必要とします。したがって、最初の項は $\mathbf{w}$ または $b$ に依存しないため、無視できます。これで、第 2 項は、乗法定数 $\frac{1}{\sigma^2}$ を除き、前に紹介した二乗誤差損失と同じです。幸いなことに、このソリューションは$\sigma$には依存しません。したがって、平均二乗誤差を最小化することは、加法性ガウスノイズを仮定した場合の線形モデルの最尤推定と同等です。 

## 線形回帰からディープネットワークへ

ここまでは、線形モデルについてのみ説明しました。ニューラルネットワークはより豊富なモデルファミリーをカバーしていますが、線形モデルをニューラルネットワークの言語で表現することで、ニューラルネットワークと考えることができます。まず、「レイヤー」表記で書き直すところから始めましょう。 

### ニューラルネットワークダイアグラム

ディープラーニングの実践者は、モデルで起きていることを視覚化するために図を描くのが好きです。:numref:`fig_single_neuron` では、線形回帰モデルをニューラルネットワークとして表現しています。これらのダイアグラムは、各入力が出力にどのように接続されているかなどの接続性パターンを強調していますが、重みやバイアスがとる値は強調していないことに注意してください。 

![Linear regression is a single-layer neural network.](../img/singleneuron.svg)
:label:`fig_single_neuron`

:numref:`fig_single_neuron` に示すニューラルネットワークでは、入力は $x_1, \ldots, x_d$ なので、入力層の*入力数* (または*特徴次元*) は $d$ です。:numref:`fig_single_neuron` のネットワークの出力は $o_1$ なので、出力層の*出力数* は 1 です。入力値はすべて*与えられる* で、*computed* ニューロンは 1 つだけであることに注意してください。計算が行われる場所に注目して、従来、レイヤーを数えるときに入力レイヤーは考慮しません。つまり、:numref:`fig_single_neuron` のニューラルネットワークの *層数* は 1 です。線形回帰モデルは、単一の人工ニューロンだけで構成されるニューラルネットワーク、または単層ニューラルネットワークと考えることができます。 

線形回帰では、すべての入力がすべての出力 (この場合は出力が 1 つだけ) に接続されるため、この変換 (:numref:`fig_single_neuron` の出力層) は*完全結合層* または*高密度層* と見なすことができます。次の章では、このような層で構成されるネットワークについてさらに詳しく説明します。 

### 生物学

1795年に考案された線形回帰は計算神経科学よりも前から存在するため、線形回帰をニューラルネットワークと表現するのは時代錯誤のように思えるかもしれません。サイバネティスト/神経生理学者のウォーレン・マカロックとウォルター・ピッツが人工ニューロンのモデルを開発し始めたとき、線形モデルが自然に始まった理由を理解するために、:numref:`fig_Neuron`の生物学的ニューロンの漫画的な図を考えてみましょう。
*樹状突起* (入力端子)
*nucleus* (CPU)、*axon* (出力線)、*axon端子* (出力端子) により、*シナプス*を介して他のニューロンに接続できます。 

![The real neuron.](../img/neuron.svg)
:label:`fig_Neuron`

他のニューロン（または網膜などの環境センサー）から届く情報$x_i$は、樹状突起で受信されます。特に、その情報は、入力の効果（例えば、製品 $x_i w_i$ による活性化または阻害）を決定する*シナプス重み* $w_i$によって重み付けされる。複数のソースから到着する重み付けされた入力は、重み付き合計 $y = \sum_i x_i w_i + b$ として核に集約され、この情報は軸索 $y$ でさらに処理するために送信されます。通常、$\sigma(y)$ を介した非線形処理が実施されます。そこから目的地（筋肉など）に到達するか、樹状突起を介して別のニューロンに供給されます。 

確かに、そのような多くのユニットを適切な接続性と適切な学習アルゴリズムと組み合わせて、1つのニューロンだけで表現できるよりもはるかに面白くて複雑な動作を生み出すことができるという高レベルのアイデアは、実際の生物学的ニューラルシステムの研究によるものです。 

同時に、今日のディープラーニングの研究のほとんどは、神経科学に直接的なインスピレーションを与えることはほとんどありません。私たちはスチュアート・ラッセルとピーター・ノーヴィグを呼びます。彼らは古典的なAIの教科書で
*人工知能: A Modern Approach* :cite:`Russell.Norvig.2016`
飛行機は鳥に触発されたかもしれないが、鳥類学は何世紀にもわたって航空学の革新の主要な推進力ではなかったと指摘した。同様に、最近のディープラーニングのインスピレーションは、数学、統計、コンピューターサイエンスから同等かそれ以上得られています。 

## [概要

* 機械学習モデルの重要な要素は、トレーニングデータ、損失関数、最適化アルゴリズム、そして明らかにモデルそのものです。
* ベクトル化すると、すべてがより良くなり（ほとんどが数学）、より速くなります（主にコード）。
* 目的関数を最小化することと最尤推定を実行することも、同じ意味を持つことがあります。
* 線形回帰モデルもニューラルネットワークです。

## 演習

1. $x_1, \ldots, x_n \in \mathbb{R}$ というデータがあると仮定します。私たちの目標は、$\sum_i (x_i - b)^2$ が最小化されるような定数 $b$ を見つけることです。
    1. $b$ の最適値に対する解析解を求めます。
    1. この問題とその解は正規分布とどのように関係していますか。
1. 二乗誤差をもつ線形回帰の最適化問題に対する解析解を導き出します。単純化するために、バイアス $b$ を問題から省略できます (すべてが 1 で構成される $\mathbf X$ に 1 つの列を追加することで、原則的にこれを行うことができます)。
    1. 最適化問題を行列とベクトル表記で書き出します (すべてのデータを 1 つの行列として扱い、すべてのターゲット値を 1 つのベクトルとして扱います)。
    1. $w$ に対する損失の勾配を計算します。
    1. 勾配をゼロに設定し、行列方程式を解くことで解析解を求めます。
    1. 確率的勾配降下法を使用するよりもこれが良いのはいつですか？この方法が壊れるのはいつですか？
1. 加法性ノイズ $\epsilon$ を支配するノイズモデルが指数分布であると仮定します。つまり、$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$ です。
    1. モデル $-\log P(\mathbf y \mid \mathbf X)$ のデータの負の対数尤度を書き出します。
    1. クローズドフォームのソリューションを見つけられますか？
    1. この問題を解決するために、確率的勾配降下法アルゴリズムを提案する。何がうまくいかない可能性がありますか（ヒント：パラメータを更新し続けると、静止点の近くで何が起こりますか）。これを直せる？

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/40)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/258)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/259)
:end_tab:
