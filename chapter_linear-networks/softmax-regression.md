# ソフトマックス回帰
:label:`sec_softmax`

:numref:`sec_linear_regression` では線形回帰を導入し、:numref:`sec_linear_scratch` では実装をゼロから実行し、:numref:`sec_linear_concise` ではディープラーニングフレームワークの高レベル API を使用して手間のかかる作業を行いました。 

回帰は、私たちが答えたいときに手を伸ばすハンマーです。*いくら？* または *いくつですか？* 質問。家を売る際のドル (価格)、野球チームの勝利数、患者が退院するまでの入院日数を予測したい場合は、おそらく回帰モデルを探しているでしょう。 

実際には、私たちはより頻繁に*分類*に関心があります。「どれだけ」ではなく「どれだけ」を尋ねるかです。 

* このメールは迷惑メールフォルダと受信トレイのどちらにありますか？
* この顧客は、サブスクリプションサービスに「サインアップする」可能性が高い、または「サインアップしない」可能性が高いですか？
* この画像はロバ、犬、猫、または雄鶏を描いていますか？
* アストンが次に観る可能性が最も高い映画はどれですか？

口語的に言うと、機械学習の実践者は、2つの微妙に異なる問題を説明するために、*classification* という単語をオーバーロードします。(i) カテゴリ (クラス) への例のハードアサインメントのみに関心がある問題、および (ii) ソフトアサインメントを行いたい問題、つまり、以下の確率を評価する問題各カテゴリーが適用されます。ハードな割り当てだけを考えている場合でも、ソフト割り当てを行うモデルを使用することが多いため、区別が曖昧になりがちです。 

## 分類問題
:label:`subsec_classification-problem`

足を濡らすために、簡単な画像分類問題から始めましょう。ここで、各入力は $2\times2$ グレースケールイメージで構成されています。各ピクセル値を 1 つのスカラーで表すことができ、4 つの特徴 $x_1, x_2, x_3, x_4$ が得られます。さらに、それぞれの画像が「猫」、「鶏」、「犬」のいずれかのカテゴリに属すると仮定します。 

次に、ラベルの表現方法を選択する必要があります。明らかな選択肢が2つあります。おそらく最も自然な衝動は $y \in \{1, 2, 3\}$ を選択することでしょう。ここで、整数はそれぞれ $\{\text{dog}, \text{cat}, \text{chicken}\}$ を表します。これは、そのような情報をコンピューターに「保存」する優れた方法です。カテゴリに自然順序付けがある場合、たとえば $\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$ を予測しようとした場合、この問題を回帰としてキャストし、ラベルをこの形式で保持するのが理にかなっているかもしれません。 

しかし、一般的な分類問題には、クラス間の自然な順序付けは伴いません。幸いなことに、統計学者ははるか昔にカテゴリカルデータを表現する簡単な方法、*ワンホットエンコーディング*を発明しました。ワンホットエンコーディングは、カテゴリと同じ数の成分をもつベクトルです。特定のインスタンスのカテゴリに対応するコンポーネントは 1 に設定され、その他すべてのコンポーネントは 0 に設定されます。この例では、ラベル $y$ は 3 次元ベクトルで、$(1, 0, 0)$ は「cat」、$(0, 1, 0)$ は「鶏」、$(0, 0, 1)$ は「犬」に対応します。 

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

## ネットワークアーキテクチャ

考えられるすべてのクラスに関連する条件付き確率を推定するには、クラスごとに 1 つずつ、複数の出力をもつモデルが必要です。線形モデルによる分類に対処するには、出力の数だけアフィン関数が必要になります。各出力は独自のアフィン関数に対応します。この例では、4 つの特徴量と 3 つの可能な出力カテゴリがあるため、重みを表すには 12 個のスカラー (添字付き $w$)、バイアスを表すには 3 つのスカラー (添字付き $b$) が必要です。入力ごとに、次の 3 つの*logit*、$o_1, o_2$、$o_3$ を計算します。 

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

この計算は :numref:`fig_softmaxreg` に示すニューラルネットワークダイアグラムで表すことができます。線形回帰と同様に、ソフトマックス回帰も単層ニューラルネットワークです。また、各出力 $o_1, o_2$ および $o_3$ の計算は $x_1$、$x_2$、$x_3$、$x_4$ のすべての入力に依存するため、ソフトマックス回帰の出力層は全結合層としても記述できます。 

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

モデルをよりコンパクトに表現するために、線形代数表記法を使うことができます。ベクトル形式では $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$ に到達しました。これは、数学とコードの記述の両方により適した形式です。すべての重みを $3 \times 4$ 行列に集め、与えられたデータ例 $\mathbf{x}$ の特徴量に対して、出力は、重みと入力特徴量にバイアス $\mathbf{b}$ を加えた行列-ベクトル積で与えられることに注意してください。 

## 全結合層のパラメータ化コスト
:label:`subsec_parameterization-cost-fc-layers`

以降の章で説明するように、完全結合層はディープラーニングのいたるところに存在しています。ただし、その名前が示すように、全結合層は潜在的に多くの学習可能なパラメーターと「完全に」接続されています。具体的には、入力が $d$、出力が $q$ の完全接続レイヤでは、パラメータ化のコストは $\mathcal{O}(dq)$ となり、実際には非常に高くなる可能性があります。幸いなことに、$d$ の入力を $q$ 出力に変換するコストは $\mathcal{O}(\frac{dq}{n})$ にまで削減できます。この場合は、ハイパーパラメータ $n$ を柔軟に指定して、実際のアプリケーション :cite:`Zhang.Tay.Zhang.ea.2021` でパラメーターの節約とモデルの有効性のバランスをとることができます。 

## ソフトマックスオペレーション
:label:`subsec_softmax_operation`

ここで取り上げる主なアプローチは、モデルの出力を確率として解釈することです。観測されたデータの尤度を最大化する確率を生成するために、パラメーターを最適化します。次に、予測を生成するために、予測確率が最大であるラベルを選択するなど、しきい値を設定します。 

正式には、任意の出力 $\hat{y}_j$ を、与えられた項目がクラス $j$ に属する確率として解釈するようにします。次に、予測値 $\operatorname*{argmax}_j y_j$ として出力値が最も大きいクラスを選択できます。たとえば、$\hat{y}_1$、$\hat{y}_2$、$\hat{y}_3$ がそれぞれ 0.1、0.8、0.1 である場合、カテゴリ 2 が予測されます。このカテゴリは (この例では)「ニワトリ」を表します。 

ロジット $o$ を目的の出力として直接解釈するように提案したくなるかもしれません。ただし、線形層の出力を確率として直接解釈することには、いくつかの問題があります。一方では、これらの数値の合計を1に制限するものは何もありません。一方、入力によっては、負の値を取ることもあります。これらは :numref:`sec_prob` で示された確率の基本公理に違反します 

出力を確率として解釈するには、(新しいデータであっても) 非負で合計が 1 になることを保証しなければなりません。さらに、モデルが確率を忠実に推定するように促すトレーニング目標も必要です。分類器が 0.5 を出力するすべてのインスタンスのうち、これらの例の半分が実際に予測されるクラスに属することを期待しています。これは*Calibration* と呼ばれるプロパティです。 

1959年に社会科学者のR. Duncan Luceが*選択モデル*の文脈で考案した*softmax関数*は、まさにこれを行います。モデルが微分可能であることを要求しながら、ロジットを非負にして合計が 1 になるようにロジットを変換するには、まず各ロジットをべき乗し (非負性を保証して)、その合計で除算します (合計が 1 になるようにします)。 

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{where}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}. $$
:eqlabel:`eq_softmax_y_and_o`

$j$は$0 \leq \hat{y}_j \leq 1$で$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$を見るのは簡単です。したがって、$\hat{\mathbf{y}}$ は適切な確率分布であり、要素値をそれに応じて解釈できます。softmax 演算は、各クラスに割り当てられる確率を決定するソフトマックス前の値であるロジット $\mathbf{o}$ の順序を変更しないことに注意してください。したがって、予測中に、最も可能性の高いクラスを次の方法で選択できます。 

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

softmax は非線形関数ですが、ソフトマックス回帰の出力は入力特徴量のアフィン変換によって *決定* されます。したがって、ソフトマックス回帰は線形モデルです。 

## ミニバッチのベクトル化
:label:`subsec_softmax_vectorization`

計算効率を向上し、GPU を活用するために、通常、データのミニバッチに対してベクトル計算を実行します。特徴次元 (入力数) $d$、バッチサイズが $n$ の例のミニバッチ $\mathbf{X}$ が与えられていると仮定します。さらに、出力に $q$ のカテゴリがあると仮定します。この場合、ミニバッチフィーチャー $\mathbf{X}$ は $\mathbb{R}^{n \times d}$ になり、重みは $\mathbf{W} \in \mathbb{R}^{d \times q}$ になり、バイアスは $\mathbf{b} \in \mathbb{R}^{1\times q}$ を満たします。 

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

これにより、一回に 1 つの例を処理した場合に実行する行列とベクトルの積に対して、行列-行列の積 $\mathbf{X} \mathbf{W}$ に対する支配的な演算が高速化されます。$\mathbf{X}$ の各行はデータ例を表すため、softmax 演算自体は *rowwise* で計算できます。$\mathbf{O}$ の各行に対して、すべてのエントリをべき乗し、合計で正規化します。:eqref:`eq_minibatch_softmax_reg` の合計 $\mathbf{X} \mathbf{W} + \mathbf{b}$ の間にブロードキャストをトリガーすると、ミニバッチロジット $\mathbf{O}$ と出力確率 $\hat{\mathbf{Y}}$ はどちらも $n \times q$ 行列になります。 

## 損失関数

次に、予測される確率の質を測定する損失関数が必要です。最尤推定に頼ります。これは、線形回帰で平均二乗誤差の目的の確率的正当化を提供するときに遭遇したのとまったく同じ概念です (:numref:`subsec_normal_distribution_and_squared_loss`)。 

### 対数尤度

softmax 関数からベクトル $\hat{\mathbf{y}}$ が得られ、任意の入力 $\mathbf{x}$ が与えられた場合、各クラスの推定条件付き確率として解釈できます (例:$\hat{y}_1$ = $P(y=\text{cat} \mid \mathbf{x})$)。データセット $\{\mathbf{X}, \mathbf{Y}\}$ 全体に $n$ の例があり、$i$ によってインデックス付けされた例は、特徴ベクトル $\mathbf{x}^{(i)}$ とワンホットラベルベクトル $\mathbf{y}^{(i)}$ で構成されているとします。次の特徴を考慮して、モデルに従って実際のクラスがどの程度確率が高いかを確認することで、推定値を現実と比較できます。 

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

最尤推定によると、$P(\mathbf{Y} \mid \mathbf{X})$ は最大化されます。これは、負の対数尤度を最小化することに相当します。 

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

ここで、$q$ クラスに対するラベル $\mathbf{y}$ とモデル予測 $\hat{\mathbf{y}}$ のペアについて、損失関数 $l$ は次のようになります。 

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

後述する理由から、:eqref:`eq_l_cross_entropy` の損失関数は一般に*クロスエントロピー損失* と呼ばれます。$\mathbf{y}$ は長さ $q$ の 1 ホットベクトルなので、1 つの項を除くすべての座標 $j$ の合計は消失します。$\hat{y}_j$ はすべて予測確率であるため、その対数は $0$ より大きくなることはありません。したがって、実際のラベルを*確実性*で正しく予測した場合、つまり実際のラベル $\mathbf{y}$ の予測確率 $P(\mathbf{y} \mid \mathbf{x}) = 1$ であれば、損失関数をこれ以上最小化することはできません。これは不可能な場合が多いことに注意してください。たとえば、データセットにラベルノイズがある可能性があります (一部の例では誤ったラベルが付けられている可能性があります)。また、入力フィーチャの情報量が十分でないため、すべての例を完全に分類できない場合もあります。 

### ソフトマックスとデリバティブ
:label:`subsec_softmax_and_derivatives`

ソフトマックスとそれに対応する損失は非常に一般的であるため、計算方法をもう少しよく理解する価値があります。:eqref:`eq_softmax_y_and_o` を :eqref:`eq_l_cross_entropy` の損失の定義に差し込み、得られたソフトマックスの定義を使用します。 

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

何が起こっているのかをもう少しよく理解するために、ロジット $o_j$ に関する微分を考えてみましょう。我々が得る 

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

言い換えると、微分とは、ソフトマックス演算で表されるモデルによって割り当てられた確率と、ワンホットラベルベクトルの要素によって表される実際の発生との差です。この意味で、これは回帰で見られたものと非常に似ています。勾配は観測値 $y$ と推定 $\hat{y}$ の差でした。これは偶然ではない。指数族 ([online appendix on distributions](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html) を参照) モデルでは、対数尤度の勾配は正確にこの項によって与えられます。この事実により、実際には勾配の計算が容易になります。 

### クロスエントロピー損失

ここで、単一の結果だけでなく、結果全体の分布を観察する場合を考えてみましょう。ラベル $\mathbf{y}$ には、以前と同じ表現を使用できます。唯一の違いは、$(0, 0, 1)$ などのバイナリエントリのみを含むベクトルではなく、$(0.1, 0.2, 0.7)$ などの一般的な確率ベクトルがあることです。:eqref:`eq_l_cross_entropy` で損失 $l$ を定義するために以前に使用した計算は、解釈が少し一般的であるというだけで、まだうまく機能します。これは、ラベル上の分布に対する損失の期待値です。この損失は*クロスエントロピー損失* と呼ばれ、分類問題で最も一般的に使用される損失の 1 つです。情報理論の基礎を紹介するだけで、名前の謎を解き明かすことができます。情報理論の詳細について理解したい場合は、[online appendix on information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html) を参照してください。 

## 情報理論の基礎
:label:`subsec_info_theory_basics`

*情報理論*は、符号化、復号化、送信、
また、情報 (データとも呼ばれる) をできるだけ簡潔な形で操作します。 

### エントロピー

情報理論の中心的な考え方は、データに含まれる情報量を定量化することです。この量は、データの圧縮能力に厳しい制限を課します。情報理論では、この量は分布 $P$ の*エントロピー* と呼ばれ、次の方程式で捉えられます。 

$$H[P] = \sum_j - P(j) \log P(j).$$
:eqlabel:`eq_softmax_reg_entropy`

情報理論の基本定理の一つに、分布 $P$ から無作為に抽出されたデータをエンコードするには、少なくとも $H[P]$「nats」が必要であるとされています。「nat」が何であるか疑問に思うなら、それはビットと同等ですが、基数2のコードではなく基数$e$のコードを使用する場合です。したがって、1 つの NAT は $\frac{1}{\log(2)} \approx 1.44$ ビットになります。 

### サプライザル

圧縮が予測とどのような関係があるのか疑問に思われるかもしれません。圧縮したいデータのストリームがあるとします。次のトークンを予測することが常に容易であれば、このデータは簡単に圧縮できます。ストリーム内のすべてのトークンが常に同じ値を取るという極端な例を考えてみましょう。それは非常に退屈なデータストリームです！退屈なだけでなく、予測も簡単です。それらは常に同じなので、ストリームの内容を伝えるために情報を送信する必要はありません。予測しやすく、圧縮も簡単です。 

しかし、すべての出来事を完全に予測できなければ、驚くこともあります。イベントに低い確率を割り当てると、驚きはより大きくなります。クロード・シャノンは、事象$j$を (主観的な) 確率$P(j)$に割り当てた事象を観測したときの*驚き*を定量化するために、$\log \frac{1}{P(j)} = -\log P(j)$に落ち着いた。:eqref:`eq_softmax_reg_entropy` で定義されているエントロピーは、データ生成プロセスに真に合致する正しい確率を割り当てたときの「予想される驚き」になります。 

### クロスエントロピー再考

したがって、エントロピーが真の確率を知っている人が経験する驚きのレベルであれば、クロスエントロピーとは何か疑問に思うかもしれません。$H(P, Q)$ と表記される* $P$ *から* $Q$ へのクロスエントロピーは、主観的確率 $Q$ をもつ観測者が、確率 $P$ に従って実際に生成されたデータを見たときに予想される驚きです。$P=Q$ のときには、可能な限り低いクロスエントロピーが達成されます。この場合、$P$ から $Q$ までのクロスエントロピーは $H(P, P)= H(P)$ になります。 

要するに、クロスエントロピー分類の目的は、(i) 観測されるデータの尤度を最大化すること、(ii) ラベルを伝達するのに必要な驚き (したがってビット数) を最小化することの 2 つの方法で考えることができます。 

## モデル予測と評価

ソフトマックス回帰モデルに学習をさせた後、特徴の例があれば、各出力クラスの確率を予測できます。通常、予測される確率が最も高いクラスを出力クラスとして使用します。実際のクラス (ラベル) と一致していれば、予測は正しいです。実験の次のパートでは、*accuracy* を使用してモデルの性能を評価します。これは、正しい予測の数と予測の総数の比率に等しくなります。 

## [概要

* softmax 演算はベクトルを受け取り、確率にマップします。
* Softmax 回帰は分類問題に適用されます。softmax 演算で出力クラスの確率分布を使用します。
* クロスエントロピーは、2 つの確率分布の差の優れた尺度です。モデルで与えられたデータをエンコードするのに必要なビット数を測定します。

## 演習

1. 指数群とソフトマックスの関係をもう少し詳しく調べることができます。
    1. ソフトマックスに対するクロスエントロピー損失 $l(\mathbf{y},\hat{\mathbf{y}})$ の 2 次導関数を計算します。
    1. $\mathrm{softmax}(\mathbf{o})$ で与えられる分布の分散を計算し、上記で計算した 2 次導関数と一致することを示します。
1. 等しい確率で発生するクラスが 3 つあると仮定します。つまり、確率ベクトルは $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$ です。
    1. バイナリコードを設計しようとすると何が問題になりますか？
    1. もっと良いコードをデザインできますか？ヒント:2つの独立したオブザベーションをエンコードしようとするとどうなりますか？$n$ 個の観測値を一緒にエンコードするとどうなるでしょうか。
1. Softmax は、上で紹介したマッピングの誤称です (ただし、ディープラーニングでは誰もがこれを使用しています)。実際のソフトマックスは $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$ と定義されています。
    1. $\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$であることを証明しろ
    1. $\lambda > 0$という条件で、これが$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$に当てはまることを証明してください。
    1. $\lambda \to \infty$には$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$があることを示してください。
    1. ソフトミンはどんな感じですか？
    1. これを 3 つ以上の数値に拡張します。

[Discussions](https://discuss.d2l.ai/t/46)
