# 最新のリカレントニューラルネットワーク
:label:`chap_modern_rnn`

配列データをよりよく扱えるRNNの基本について紹介しました。デモンストレーションのために、テキストデータに RNN ベースの言語モデルを実装しました。しかし、このような手法は、今日、幅広いシーケンス学習の問題に直面している実践者にとっては十分ではないかもしれません。 

例えば、実際に注目すべき問題は、RNNの数値的不安定性である。勾配クリッピングなどの実装手法を適用してきましたが、この問題はシーケンスモデルのより洗練された設計によってさらに軽減できます。具体的には、ゲート型RNNは実際にははるかに一般的です。まず、このような広く使われているネットワークのうち、*ゲーテッドリカレントユニット* (GRU) と*長期短期記憶* (LSTM) の2つを導入することから始めます。さらに、これまで議論されてきた単一方向隠れ層でRNNアーキテクチャを拡張する。複数の隠れ層をもつディープ・アーキテクチャーについて説明し、順方向および逆方向リカレント計算の両方による双方向設計について議論する。このような拡張は、現代のリカレントネットワークで頻繁に採用されています。これらの RNN バリアントを説明する際には、:numref:`chap_rnn` で導入されたのと同じ言語モデリング問題を引き続き検討します。 

実際、言語モデリングでは、シーケンス学習ができることのほんの一部しか明らかになりません。自動音声認識、テキスト読み上げ、機械翻訳など、さまざまなシーケンス学習の問題では、入力と出力の両方が任意の長さのシーケンスになります。このタイプのデータをどのように適合させるかを説明するために、機械翻訳を例にとり、RNNとビームサーチに基づくエンコーダ/デコーダアーキテクチャを紹介します。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
