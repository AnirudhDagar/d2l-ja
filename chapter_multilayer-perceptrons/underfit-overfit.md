# モデル選択、アンダーフィット、オーバーフィット
:label:`sec_model_selection`

機械学習の科学者として、私たちの目標は「パターン」を発見することです。しかし、単にデータを記憶したのではなく、本当に*一般的な*パターンを発見したと確信するにはどうすればよいでしょうか。たとえば、患者を認知症の状態に結びつける遺伝子マーカーのパターンを探したいとします。ラベルはセット $\{\text{dementia}, \text{mild cognitive impairment}, \text{healthy}\}$ から描かれています。各人の遺伝子はそれらを一意に（同一の兄弟を無視して）識別するため、データセット全体を記憶することができます。 

モデルに言わせたくない
*「あれはボブだ！彼を覚えてる！彼は認知症だ！」*
理由は単純です。将来モデルを展開すると、そのモデルがこれまでに見たことのない患者に遭遇することになります。私たちの予測は、モデルが本当に*一般的な*パターンを発見した場合にのみ役に立ちます。 

より正式に要約するために、私たちの目標は、トレーニングセットが引き出された基礎となる母集団の規則性を捉えるパターンを発見することです。この取り組みが成功すれば、これまで遭遇したことのない個人に対してもリスク評価を成功させることができます。この問題、つまり*一般化*するパターンをどうやって発見するかが、機械学習の根本的な問題です。 

危険なのは、モデルをトレーニングするときに、ごくわずかなデータサンプルにしかアクセスしないことです。最大のパブリック画像データセットには、約 100 万枚の画像が含まれています。多くの場合、数千または数万のデータ例からしか学ばないといけません。大規模な病院システムでは、何十万もの医療記録にアクセスする可能性があります。有限サンプルを扱う場合、より多くのデータを収集しても保持されない明らかな関連性が発見されるリスクがあります。 

基礎となる分布に近似するよりも学習データを近似する現象を*overfitting*、過剰適合に対抗するために使用される手法を*正則化* と呼びます。前のセクションでは、Fashion-MNIST データセットを試しているときに、この影響を観察したことがあるかもしれません。実験中にモデル構造またはハイパーパラメーターを変更した場合、十分なニューロン、層、およびトレーニングエポックがあれば、テストデータの精度が低下しても、最終的にモデルはトレーニングセットで完全な精度に達する可能性があることに気付いたかもしれません。 

## 学習誤差と汎化誤差

この現象をより形式的に論じるためには、学習誤差と汎化誤差を区別する必要があります。*トレーニング誤差* は、トレーニングデータセットで計算されたモデルの誤差です。*汎化誤差* は、元のサンプルと同じ基になるデータ分布から引き出された追加のデータ例の無限ストリームに適用した場合のモデルの誤差の予測値です。 

問題として、汎化誤差を正確に計算することはできません。これは、無限データのストリームが架空のオブジェクトだからです。実際には、学習セットから除外されたランダムなデータ例で構成される独立したテストセットにモデルを適用して、汎化誤差を「推定」する必要があります。 

次の3つの思考実験は、この状況をよりよく説明するのに役立ちます。最終試験の準備をしようとしている大学生を考えてみましょう。勤勉な学生は、前年度の試験を使用して、よく練習し、能力をテストするよう努めます。それにもかかわらず、過去の試験でうまくやることは、それが重要なときに彼が優れていることを保証するものではありません。たとえば、学生は試験問題の解答を暗記して準備しようとするかもしれません。そのためには、学生は多くのことを暗記する必要があります。彼女は過去の試験の答えを完全に覚えているかもしれません。他の学生は、特定の答えを出す理由を理解しようとすることによって準備するかもしれません。ほとんどの場合、後者の学生の方がはるかに優れています。 

同様に、単にルックアップテーブルを使用して質問に答えるモデルを考えてみましょう。許容される入力のセットが離散的で適度に小さい場合、おそらく*多くの*トレーニング例を見た後であれば、このアプローチはうまく機能します。それでも、このモデルには、これまでに見たことのない例に直面したときに、ランダムな推測よりも優れた機能はありません。実際には、入力スペースが大きすぎて、考えられるすべての入力に対応する答えを記憶できません。たとえば、白黒の $28\times28$ イメージについて考えてみます。各ピクセルが $256$ のグレースケール値の 1 つを取ることができれば、$256^{784}$ 個のイメージが考えられます。つまり、低解像度のグレースケールサムネイルサイズの画像は、宇宙の原子よりもはるかに多いということです。そのようなデータに遭遇したとしても、ルックアップテーブルを格納する余裕はありませんでした。 

最後に、コイントス (クラス 0: 頭、クラス 1: テール) の結果を、利用可能な状況に応じた特徴に基づいて分類しようとする問題を考えてみましょう。コインが公正であると仮定します。どんなアルゴリズムを考えても、汎化誤差は常に $\frac{1}{2}$ になります。しかし、ほとんどのアルゴリズムでは、たとえ特徴がなくても、抽選の運にもよりますが、トレーニングエラーはかなり低くなると予想されます。データセット {0, 1, 1, 0, 1} を考えてみましょう。私たちの特徴のないアルゴリズムは、限られたサンプルから*1*と思われる*マジョリティクラス*を常に予測することに頼らなければなりません。この場合、クラス 1 を常に予測するモデルでは $\frac{1}{3}$ の誤差が発生します。これは、汎化誤差よりもはるかに優れています。データ量を増やすと、頭部の割合が $\frac{1}{2}$ から大幅に逸脱する確率は小さくなり、学習誤差は汎化誤差と一致するようになります。 

### 統計的学習理論

一般化は機械学習の根本的な問題なので、多くの数学者や理論家が、この現象を説明する形式理論の開発に全力を注いでいることを知っても驚くことではないかもしれません。GlivenkoとCantelliは [同義の定理](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem) で、学習誤差が汎化誤差に収束する速度を導き出しました。一連の独創的な論文の中で、[Vapnik and Chervonenkis](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory) はこの理論をより一般的な関数クラスに拡張した。この研究は統計的学習理論の基礎を築いた。 

これまで取り上げた標準の教師あり学習の設定では、本書のほとんどの部分で取り上げますが、学習データとテストデータの両方が、*同一* の分布から「独立して」引き出されると仮定します。これは一般に「*i.i.d. Assumption*」と呼ばれ、データをサンプリングするプロセスにはメモリがないことを意味します。言い換えれば、描かれた2番目の例と3番目の描かれた例は、描かれた2番目と200万番目のサンプルと相関関係がありません。 

優れた機械学習の科学者になるには、批判的に考える必要があります。すでにこの仮定に穴を開けて、仮定が失敗する一般的なケースを考え出す必要があります。UCSF Medical Centerで患者から収集したデータに基づいて死亡リスク予測因子をトレーニングし、マサチューセッツ総合病院の患者に適用するとどうなるでしょうか？これらの分布はまったく同じではありません。さらに、抽選は時間的に相関する可能性があります。ツイートのトピックを分類するとどうなりますか？ニュースサイクルは、議論されているトピックに一時的な依存関係を作成し、独立性の前提に違反します。 

I.D. の仮定の軽微な違反から逃れることもあり、私たちのモデルは非常にうまく機能し続けるでしょう。結局のところ、ほとんどすべての現実世界のアプリケーションには、i.d. の前提に対する軽微な違反が含まれていますが、顔認識、音声認識、言語翻訳など、さまざまなアプリケーションに役立つツールが多数あります。 

他の違反は必ずトラブルの原因となります。たとえば、顔認識システムを大学生だけにトレーニングしてトレーニングし、老人ホームの高齢者を監視するツールとして導入するとします。大学生は高齢者とかなり違って見える傾向があるため、これはうまく機能しそうにありません。 

以降の章では、i.i.d. 仮定の違反から生じる問題について説明します。今のところ、i.d. の仮定を当たり前のことと考えても、汎化を理解することは手ごわい問題です。さらに、ディープニューラルネットワークが一般化するのと同様に一般化する理由を説明するかもしれない正確な理論的基礎を解明することは、学習理論の最大の心を悩ませ続けています。 

モデルをトレーニングするときは、トレーニングデータにできる限り適合する関数を探そうとします。関数が非常に柔軟で、真の関連性と同じくらい簡単に偽のパターンに追いつくことができる場合、目に見えないデータに対して適切に一般化するモデルを生成しなくても、*うまく機能しすぎます*。これはまさに、避けたい、または少なくとも制御したいことです。ディープラーニングの手法の多くは、過適合を防ぐことを目的としたヒューリスティックとトリックです。 

### モデルの複雑さ

単純なモデルと豊富なデータがある場合、汎化誤差は学習誤差に似ていると予想されます。より複雑なモデルと少数の例を扱うと、学習誤差は減少するが、汎化ギャップは大きくなると予想されます。モデルの複雑さを正確に構成しているのは複雑な問題です。モデルが適切に一般化されるかどうかは、多くの要因によって決まります。たとえば、パラメーターが多いモデルはより複雑であると見なされることがあります。パラメーターがより広い範囲の値をとることができるモデルは、より複雑になる場合があります。ニューラルネットワークでは、トレーニングの反復数が多いモデルはより複雑で、*早期停止* (トレーニングの反復回数が少ない) モデルはより複雑ではないと考えることがよくあります。 

実質的に異なるモデルクラス (たとえば、決定木とニューラルネットワーク) のメンバー間で複雑さを比較するのは難しい場合があります。今のところ、単純な経験則は非常に有用です。恣意的な事実を簡単に説明できるモデルは統計学者が複雑と見なすものですが、表現力は限られていてもデータをうまく説明できるモデルはおそらく真実に近いでしょう。哲学では、これは科学理論の偽造可能性に関するポッパーの基準と密接に関連しています。理論は、データに適合し、それを反証するために使用できる特定のテストがある場合に適しています。すべての統計的推定は重要であるため、これは重要です。
*ポストホック*、
つまり、事実を観察した後に推定するため、関連する誤謬に対して脆弱です。今のところ、私たちは哲学を脇に置き、より具体的な問題に固執します。 

このセクションでは、直感的に理解できるように、モデルクラスの一般化可能性に影響するいくつかの要因に焦点を当てます。 

1. 調整可能なパラメーターの数。*自由度* と呼ばれることもある調整可能なパラメーターの数が多い場合、モデルは過適合の影響を受けやすくなります。
1. パラメータがとる値。重みがより広い範囲の値を取ることができる場合、モデルは過適合の影響を受けやすくなります。
1. トレーニング例の数。モデルが単純であっても、1 つまたは 2 つの例しか含まれていないデータセットを過剰適合させるのは簡単です。しかし、何百万もの例を含むデータセットを過剰適合させるには、きわめて柔軟なモデルが必要です。

## モデル選択

機械学習では、通常、いくつかの候補モデルを評価した後、最終モデルを選択します。このプロセスを*モデル選択* と呼びます。比較の対象となるモデルの性質が根本的に異なる場合があります (たとえば、決定木と線形モデル)。また、異なるハイパーパラメーター設定でトレーニングされた同じクラスのモデルのメンバーを比較する場合もあります。 

たとえば、MLP では、隠れ層の数や隠れ単位の数が異なり、各隠れ層に適用される活性化関数の選択肢が異なるモデルを比較したい場合があります。候補モデルの中から最適なモデルを決定するために、通常は検証データセットを使用します。 

### 検証データセット

原則として、すべてのハイパーパラメータを選択するまでテストセットに触れないでください。モデル選択プロセスでテストデータを使用した場合、テストデータを過剰に適合させるリスクがあります。そうすれば、私たちは深刻な問題に陥るでしょう。トレーニングデータをオーバーフィットさせた場合、正直さを保つためにテストデータに対する評価が常に行われます。しかし、テストデータを過剰に適合させたら、どうしてわかるでしょうか？ 

したがって、モデル選択にテストデータに頼るべきではありません。しかし、モデルのトレーニングに使用するデータそのものに対する汎化誤差を推定することはできないため、モデルの選択をトレーニングデータだけに頼ることはできません。 

実際のアプリケーションでは、画像が濁ります。最適なモデルを評価したり、少数のモデルを相互に比較したりするために、テストデータに一度だけ触れるのが理想的ですが、実際のテストデータは 1 回使用しただけで破棄されることはほとんどありません。実験のラウンドごとに新しいテストセットを用意することはめったにありません。 

この問題に対処するための一般的な方法は、トレーニングデータセットとテストデータセットに加えて、*検証データセット* (または*検証セット*) を組み込んで、データを 3 つの方法で分割することです。その結果、検証データとテストデータの境界があいまいなほど曖昧になるという、曖昧な習慣が生まれます。特に明記されていない限り、この本の実験では、真のテストセットを使用せずに、トレーニングデータと検証データと呼ぶべきものを実際に扱っています。したがって、本の各実験で報告される精度は、実際には検証精度であり、真のテストセットの精度ではありません。 

### $K$ 分割交差検証

トレーニングデータが不足していると、適切な検証セットを構成するのに十分なデータを保持する余裕すらできない場合があります。この問題に対する一般的な解決策の 1 つは、$K$*-fold 交差検証* を採用することです。ここでは、元のトレーニングデータが $K$ 個の重複しないサブセットに分割されます。その後、モデルのトレーニングと検証が $K$ 回実行され、そのたびに $K-1$ のサブセットでトレーニングされ、別のサブセット (そのラウンドではトレーニングに使用されないサブセット) で検証されます。最後に、$K$ 実験の結果を平均化して学習誤差と検証誤差を推定します。 

## アンダーフィットまたはオーバーフィット？

学習エラーと検証エラーを比較するときは、2 つの一般的な状況に留意する必要があります。まず、トレーニングエラーと検証エラーの両方が大きいが、両者の間にわずかなギャップがある場合に注意します。モデルがトレーニングエラーを減らすことができない場合、モデルが単純すぎる (つまり、表現力が足りない) ため、モデル化しようとしているパターンをキャプチャできない可能性があります。さらに、トレーニングエラーと検証エラーの間の「汎化ギャップ」は小さいため、より複雑なモデルで回避できると信じる理由があります。この現象を*アンダーフィット*といいます。 

一方、上で説明したように、トレーニングエラーが検証エラーよりも大幅に小さく、深刻な*オーバーフィット*を示すケースに注意する必要があります。オーバーフィットは必ずしも悪いことではないことに注意してください。特にディープラーニングでは、最良の予測モデルが、ホールドアウトデータよりもトレーニングデータの方がはるかに優れたパフォーマンスを発揮することがよく知られています。最終的には、通常、学習エラーと検証エラーのギャップよりも、検証エラーを重視します。 

過適合か不適合かは、モデルの複雑さと利用可能なトレーニングデータセットのサイズの両方に依存する可能性があります。これについては、以下で説明する 2 つのトピックです。 

### モデルの複雑さ

過適合とモデルの複雑さに関する古典的な直感を説明するために、多項式を使用した例を挙げます。1 つのフィーチャ $x$ とそれに対応する実数値のラベル $y$ で構成される学習データから、次数 $d$ の多項式を求めます。 

$$\hat{y}= \sum_{i=0}^d x^i w_i$$

ラベル$y$を見積もります。これは単なる線形回帰問題で、$x$ のべき乗によって特徴が与えられ、モデルの重みが $w_i$ で与えられ、$x^0 = 1$ 以降の $w_0$ によってバイアスが与えられます。これは単なる線形回帰問題なので、二乗誤差を損失関数として使用できます。 

高次の多項式はパラメーターが多く、モデル関数の選択範囲が広いため、高次の多項式関数は低次の多項式関数よりも複雑です。トレーニングデータセットを修正すると、高次の多項式関数は、低次多項式に比べて常に低い (最悪の場合、等しい) トレーニングエラーを達成する必要があります。実際、各データ例の値が $x$ である場合は常に、次数がデータ例の数と等しい多項式関数はトレーニングセットに完全に適合します。:numref:`fig_capacity_vs_error` では、多項式の次数と過適合と過適合の関係を可視化します。 

![Influence of model complexity on underfitting and overfitting](../img/capacity-vs-error.svg)
:label:`fig_capacity_vs_error`

### データセットサイズ

もう 1 つ留意すべき大きな考慮事項は、データセットのサイズです。モデルを修正すると、トレーニングデータセットに含まれるサンプルが少ないほど、過適合が発生する可能性が高くなります（さらに深刻になります）。学習データの量が増えるにつれて、汎化誤差は一般に減少します。さらに、一般的に、より多くのデータが害を及ぼすことはありません。固定タスクとデータ分散では、通常、モデルの複雑度とデータセットのサイズには関係があります。より多くのデータがあれば、より複雑なモデルをあてはめようとすると有益です。十分なデータがないと、単純なモデルは打ち負かすのが難しくなる可能性があります。多くのタスクにおいて、ディープラーニングは数千ものトレーニング例が利用できる場合にのみ、線形モデルよりも優れたパフォーマンスを発揮します。ディープラーニングの現在の成功の一部は、インターネット企業、安価なストレージ、コネクテッドデバイス、および経済の広範なデジタル化により、大量のデータセットが現在豊富に存在しているためです。 

## 多項式回帰

これで (**多項式をデータにあてはめることで、これらの概念を対話的に探索できます**)

```{.python .input}
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn
import math
npx.set_np()
```

```{.python .input}
#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
import numpy as np
import math
```

```{.python .input}
#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
import numpy as np
import math
```

### データセットの生成

まず、データが必要です。$x$ を指定すると、学習データとテストデータに対して [**次の三次多項式を使用してラベルを生成**] します。 

(**$$y = 5 + 1.2x-3.4\ frac {x^2} {2!}+ 5.6\ frac {x^3} {3!}+\ イプシロン\ text {where}\ イプシロン\ sim\ mathcal {N} (0, 0.1 ^2) .$$**) 

ノイズ項 $\epsilon$ は、平均 0、標準偏差 0.1 の正規分布に従います。最適化のためには、通常、非常に大きな値の勾配や損失を避けたいと考えています。これが、*フィーチャ* が $x^i$ から $\ frac {x^i} {i!} に再スケーリングされる理由です。$。これにより、大きな指数 $i$ に対して非常に大きな値を避けることができます。トレーニングセットとテストセットにそれぞれ100個のサンプルを合成します。

```{.python .input}
#@tab all
max_degree = 20  # Maximum degree of the polynomial
n_train, n_test = 100, 100  # Training and test dataset sizes
true_w = np.zeros(max_degree)  # Allocate lots of empty space
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size=(n_train + n_test, 1))
np.random.shuffle(features)
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!
# Shape of `labels`: (`n_train` + `n_test`,)
labels = np.dot(poly_features, true_w)
labels += np.random.normal(scale=0.1, size=labels.shape)
```

この場合も、`poly_features` に格納された単項式は、$\ Gamma (n) = (n-1) のガンマ関数によって再スケーリングされます。$。生成されたデータセットから [**最初の2つのサンプルを見てみましょう**]。値 1 は技術的には特徴であり、バイアスに対応する定数特徴量です。

```{.python .input}
#@tab pytorch, tensorflow
# Convert from NumPy ndarrays to tensors
true_w, features, poly_features, labels = [d2l.tensor(x, dtype=
    d2l.float32) for x in [true_w, features, poly_features, labels]]
```

```{.python .input}
#@tab all
features[:2], poly_features[:2, :], labels[:2]
```

### モデルのトレーニングとテスト

まず [**与えられたデータセットの損失を評価する関数を実装する**]。

```{.python .input}
#@tab mxnet, tensorflow
def evaluate_loss(net, data_iter, loss):  #@save
    """Evaluate the loss of a model on the given dataset."""
    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples
    for X, y in data_iter:
        l = loss(net(X), y)
        metric.add(d2l.reduce_sum(l), d2l.size(l))
    return metric[0] / metric[1]
```

```{.python .input}
#@tab pytorch
def evaluate_loss(net, data_iter, loss):  #@save
    """Evaluate the loss of a model on the given dataset."""
    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples
    for X, y in data_iter:
        out = net(X)
        y = d2l.reshape(y, out.shape)
        l = loss(out, y)
        metric.add(d2l.reduce_sum(l), d2l.size(l))
    return metric[0] / metric[1]
```

ここで [**トレーニング関数を定義する**]。

```{.python .input}
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = gluon.loss.L2Loss()
    net = nn.Sequential()
    # Switch off the bias since we already catered for it in the polynomial
    # features
    net.add(nn.Dense(1, use_bias=False))
    net.initialize()
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    test_iter = d2l.load_array((test_features, test_labels), batch_size,
                               is_train=False)
    trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': 0.01})
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data().asnumpy())
```

```{.python .input}
#@tab pytorch
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss(reduction='none')
    input_shape = train_features.shape[-1]
    # Switch off the bias since we already catered for it in the polynomial
    # features
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),
                                batch_size)
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),
                               batch_size, is_train=False)
    trainer = torch.optim.SGD(net.parameters(), lr=0.001)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data.numpy())
```

```{.python .input}
#@tab tensorflow
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = tf.losses.MeanSquaredError()
    input_shape = train_features.shape[-1]
    # Switch off the bias since we already catered for it in the polynomial
    # features
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(1, use_bias=False))
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    test_iter = d2l.load_array((test_features, test_labels), batch_size,
                               is_train=False)
    trainer = tf.keras.optimizers.SGD(learning_rate=.01)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net.get_weights()[0].T)
```

### [**三次多項式関数近似 (正規) **]

まず、データ生成関数と同じ次数である 3 次多項式関数を使用します。この結果は、このモデルの学習損失とテスト損失の両方を効果的に低減できることを示しています。学習したモデルパラメーターも真値 $w = [5, 1.2, -3.4, 5.6]$ に近くなります。

```{.python .input}
#@tab all
# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the
# polynomial features
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
```

### [**線形関数近似 (アンダーフィット) **]

一次関数近似をもう一度見てみましょう。初期のエポックが減少した後、このモデルのトレーニングロスをさらに減らすことは困難になります。最後のエポック反復が完了した後も、学習損失は依然として高いままです。非線形パターン (ここでは 3 次多項式関数のように) を近似するために使用すると、線形モデルは適合不足になりがちです。

```{.python .input}
#@tab all
# Pick the first two dimensions, i.e., 1, x, from the polynomial features
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
```

### [**高次多項式関数近似 (過適合) **]

次数が高すぎる多項式を使ってモデルをトレーニングしてみましょう。ここでは、高次係数の値がゼロに近いはずであることを知るには不十分なデータがあります。その結果、過度に複雑なモデルは非常に影響を受けやすく、トレーニングデータのノイズの影響を受けています。トレーニングロスは効果的に減らすことができますが、テストロスは依然としてはるかに高くなります。これは、複素数モデルがデータに過適合していることを示しています。

```{.python .input}
#@tab all
# Pick all the dimensions from the polynomial features
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
```

以降のセクションでは、オーバーフィットの問題と、体重の減少や脱落など、それらに対処する方法について引き続き説明します。 

## [概要

* 汎化誤差は学習誤差に基づいて推定できないため、単純に学習誤差を最小化しても必ずしも汎化誤差が減少するわけではありません。機械学習モデルでは、汎化誤差を最小限に抑えるために、過適合を防ぐよう注意する必要があります。
* 検証セットは、あまり自由に使用されない限り、モデルの選択に使用できます。
* アンダーフィットとは、モデルが学習誤差を減らすことができないことを意味します。学習誤差が検証誤差よりはるかに小さい場合、過適合が発生します。
* 適切に複雑なモデルを選択し、不十分なトレーニングサンプルを使用しないようにする必要があります。

## 演習

1. 多項式回帰問題を正確に解けますか？ヒント:線形代数を使う。
1. 多項式のモデル選択について考えてみましょう。
    1. 学習損失対モデルの複雑度 (多項式の次数) をプロットします。あなたは何を観察していますか？学習損失を0に減らすには、どの程度の多項式が必要ですか？
    1. この場合のテスト損失をプロットします。
    1. 同じプロットをデータ量の関数として生成します。
1. 正規化を落とすとどうなりますか ($1/i!$) of the polynomial features $x^i$？これを他の方法で直せる？
1. 汎化エラーがゼロになると予想できますか？

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/96)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/97)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/234)
:end_tab:
