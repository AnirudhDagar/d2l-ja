# 多層パーセプトロン
:label:`sec_mlp`

:numref:`chap_linear` では softmax 回帰 (:numref:`sec_softmax`) を導入し、アルゴリズムをゼロから実装し (:numref:`sec_softmax_scratch`)、高レベル API (:numref:`sec_softmax_concise`) を使用し、低解像度画像から 10 種類の衣類を認識するように分類器をトレーニングしました。その過程で、データをラングリングし、出力を有効な確率分布に強制し、適切な損失関数を適用し、モデルのパラメーターに関して最小化する方法を学びました。単純な線形モデルのコンテキストでこれらの力学を習得したので、この本が主に関係する比較的豊富なモデルのクラスであるディープニューラルネットワークの探索を開始できます。 

## 非表示レイヤー

:numref:`subsec_linear_model` では、バイアスによって加えられた線形変換であるアフィン変換について説明しました。はじめに、:numref:`fig_softmaxreg` に示されている softmax 回帰の例に対応するモデルアーキテクチャを思い出してください。このモデルは、単一のアフィン変換とそれに続くソフトマックス演算により、入力を出力に直接マッピングしました。ラベルがアフィン変換によって入力データと本当に関連しているなら、このアプローチで十分です。しかし、アフィン変換における直線性は「強い」仮定です。 

### 線形モデルが間違ってしまうことがある

たとえば、線形性は、*単調性*の*より弱い*仮定を意味します。特徴量が増加すると常にモデルの出力が増加するか (対応する重みが正の場合)、モデルの出力が必ず減少する (対応する重みが負の場合) 必要があります。時にはそれは理にかなっています。たとえば、個人がローンを返済するかどうかを予測しようとした場合、他のすべてを平等に保有すると、収入の高い申請者は、低所得の申請者よりも常に返済する可能性が高いと合理的に想像できます。単調ではあるが、この関係は返済の確率と直線的に関連していない可能性が高い。0万から5万への収入の増加は、100万から105万への増加よりも返済の可能性の大きい増加に相当する可能性が高い。これを処理する 1 つの方法は、たとえば収入の対数を特徴として使用して、線形性がより妥当になるようにデータを前処理することです。 

単調性に違反する例を簡単に思いつくことができることに注意してください。たとえば、体温に基づいて死亡確率を予測したいとします。体温が37°C（98.6°F）を超える人の場合、体温が高いほどリスクが高くなります。ただし、体温が37°C未満の個人では、体温が高いほどリスクが低いことを示します。この場合も、巧妙な前処理で問題を解決できるかもしれません。つまり、37°Cからの距離を特徴として使用するかもしれません。 

しかし、猫と犬の画像を分類するのはどうですか？位置 (13, 17) のピクセルの強度を上げると、イメージが犬を描写している可能性が常に高くなる (または常に減少する) べきですか?線形モデルへの依存は、猫と犬を区別するための唯一の要件は個々のピクセルの明るさを評価することであるという暗黙の仮定に対応しています。このアプローチは、画像の反転によってカテゴリが保持される世界では失敗する運命にあります。 

しかし、ここでは明らかに不条理な直線性にもかかわらず、前の例と比較して、単純な前処理の修正でこの問題に対処できることはあまり明白ではありません。これは、ピクセルの重要度が、そのコンテキスト (周囲のピクセルの値) によって複雑に左右されるためです。フィーチャ間の関連する相互作用を考慮したデータの表現が存在する可能性がありますが、その上に線形モデルが適していますが、手作業で計算する方法がわかりません。ディープニューラルネットワークでは、観測データを使用して、隠れ層を介した表現と、その表現に作用する線形予測子の両方を共同で学習しました。 

### 非表示レイヤの組み込み

1 つ以上の隠れ層を組み込むことで、線形モデルのこれらの制限を克服し、より一般的なクラスの関数を処理できます。これを行う最も簡単な方法は、完全に接続された多数のレイヤを重ねて積み重ねることです。各レイヤーは、出力が生成されるまで、その上のレイヤーにフィードされます。最初の $L-1$ 層は表現、最後の層は線形予測子と考えることができます。このアーキテクチャは一般に*マルチレイヤパーセプトロン* と呼ばれ、*MLP* と略されることもあります。以下に、MLP を図式的に示します (:numref:`fig_mlp`)。 

![An MLP with a hidden layer of 5 hidden units. ](../img/mlp.svg)
:label:`fig_mlp`

この MLP には 4 つの入力、3 つの出力があり、隠れ層には 5 つの隠れユニットが含まれています。入力層には計算が含まれないため、このネットワークで出力を生成するには、隠れ層と出力層の両方の計算を実装する必要があります。したがって、この MLP の層数は 2 になります。これらのレイヤは両方とも完全に接続されていることに注意してください。すべての入力は隠れ層のすべてのニューロンに影響し、各入力は出力層のすべてのニューロンに影響を与えます。ただし、:numref:`subsec_parameterization-cost-fc-layers` で示唆されているように、レイヤが完全接続されたMLPのパラメータ化コストは非常に高くなる可能性があり、入力または出力サイズ :cite:`Zhang.Tay.Zhang.ea.2021` を変更しなくても、パラメータ節約とモデル有効性のトレードオフにつながる可能性があります。 

### 線形から非線形へ

前述のように、行列 $\mathbf{X} \in \mathbb{R}^{n \times d}$ によって、各例に $d$ の入力 (特徴) がある $n$ 例のミニバッチを示します。隠れ層の隠れ単位が $h$ をもつ 1 つの隠れ層 MLP の場合、$\mathbf{H} \in \mathbb{R}^{n \times h}$ で表すと、隠れ層の出力は次のようになります。
*非表示リプリゼンテーション*。
数学またはコードでは、$\mathbf{H}$ は*隠れ層変数* または*隠れ変数* とも呼ばれます。隠れ層と出力層はどちらも完全に接続されているため、隠れ層の重み $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$、バイアス $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$、出力層の重み $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$、バイアス $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$ があります。正式には、1 隠れ層 MLP の出力 $\mathbf{O} \in \mathbb{R}^{n \times q}$ を次のように計算します。 

$$
\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}
$$

非表示レイヤーを追加した後、モデルでは追加のパラメーターセットを追跡および更新する必要があることに注意してください。それでは、引き換えに何が得られましたか？上で定義したモデルでは、*トラブルに対して何も得られない* ということに驚かれるかもしれません。理由は明白です。上記の隠れ単位は入力のアフィン関数によって与えられ、出力 (pre-softmax) は隠れ単位のアフィン関数にすぎません。アフィン関数のアフィン関数はそれ自体がアフィン関数です。さらに、線形モデルはすでにあらゆるアフィン関数を表現できました。 

重みの任意の値について、隠れ層を折りたたむだけで、パラメータ $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ と $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ をもつ同等の単層モデルが得られることを証明することで、同等性を公式に見ることができます。 

$$
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.
$$

多層アーキテクチャの可能性を実現するためには、アフィン変換後に隠れた各ユニットに適用する非線形*活性化関数* $\sigma$ という重要な要素をもう1つ必要とします。アクティベーション関数 ($\sigma(\cdot)$ など) の出力は*activations* と呼ばれます。一般に、アクティベーション関数が配置されると、MLP を線形モデルに折りたたむことができなくなります。 

$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$

$\mathbf{X}$ の各行は表記法の乱用を伴うミニバッチの例に対応しているため、非線形性 $\sigma$ をその入力に行単位、つまり一度に 1 つずつ適用するように定義します。:numref:`subsec_softmax_vectorization` では、softmax の表記法がローワイズ演算を表すために同じ方法で使用されたことに注意してください。多くの場合、このセクションのように、隠れ層に適用するアクティベーション関数は行単位ではなく要素単位です。つまり、レイヤーの線形部分を計算した後、他の隠れユニットがとる値を見ることなく、各アクティベーションを計算できます。これはほとんどのアクティベーション関数に当てはまります。 

より一般的な MLP を構築するために、$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$ や $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$ などの隠れ層を重ねて積み重ね続け、表現力豊かなモデルを生み出すことができます。 

### ユニバーサル近似器

MLPは、各入力の値に依存する隠れニューロンを介して、入力間の複雑な相互作用を捉えることができます。隠れノードは簡単に設計でき、例えば、一対の入力に対して基本的な論理演算など、任意の計算を実行できます。さらに、活性化関数の特定の選択肢については、MLPが普遍的近似器であることが広く知られている。単一の隠れ層ネットワークであっても、十分なノード (おそらく非常に多い) と適切な重みのセットがあれば、どの関数もモデル化できますが、実際にその関数を学習するのは難しい部分です。ニューラルネットワークは C プログラミング言語に少し似ていると考えるかもしれません。この言語は、他の現代言語と同様に、あらゆる計算可能なプログラムを表現することができます。しかし、実際にあなたの仕様に合ったプログラムを思いつくのは難しいことです。 

しかも、単一隠れ層ネットワークだから
*どんな機能も学べる*
は、単一隠れ層ネットワークに関するすべての問題を解決しようとする必要があるという意味ではありません。実際、より深い (より広い) ネットワークを使用することで、多くの関数をよりコンパクトに近似することができます。より厳密な議論については、以降の章で触れます。 

## アクティベーション関数
:label:`subsec_activation-functions`

活性化関数は、重み付けされた和を計算し、それにバイアスを加えることによって、ニューロンを活性化すべきかどうかを決定します。入力信号を出力に変換する微分可能演算子ですが、そのほとんどは非線形性を加えます。アクティベーション関数はディープラーニングの基本であるため、(**一般的なアクティベーション関数について簡単に調べてみましょう**)。

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
```

### ReLU 関数

実装のシンプルさとさまざまな予測タスクでの優れたパフォーマンスの両方から、最も一般的な選択肢は、*修正された線形単位* (*RELU*) です。[**relU は非常に単純な非線形変換を提供します**]。要素 $x$ が与えられた場合、関数はその要素の最大値と $0$ として定義されます。 

$$\operatorname{ReLU}(x) = \max(x, 0).$$

非公式には、ReLU 関数は正の要素のみを保持し、対応するアクティベーションを 0 に設定することですべての負の要素を破棄します。ある程度の直感を得るために、関数をプロットすることができます。ご覧のとおり、活性化関数は区分的線形です。

```{.python .input}
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.relu(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))
```

入力が負の場合、ReLU 関数の微分は 0 になり、入力が正の場合、ReLU 関数の微分は 1 になります。入力が正確に 0 に等しい値を取る場合、ReLU 関数は微分できないことに注意してください。このような場合、既定では左辺の微分が使用され、入力が 0 のときに微分は 0 になります。入力が実際にはゼロになることはないので、これを回避できます。微妙な境界条件が重要であれば、私たちはおそらく工学ではなく (*実際の) 数学をやっているという古い格言があります。その常識がここに当てはまるかもしれません。以下にプロットした ReLU 関数の導関数をプロットします。

```{.python .input}
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.relu(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',
         figsize=(5, 2.5))
```

ReLUを使用する理由は、その派生物が特にうまく動作するためです。ReLUは消滅するか、単に議論を通過させるかのどちらかです。これにより、最適化の動作が改善され、以前のバージョンのニューラルネットワークに悩まされていた勾配の消失という十分に文書化された問題が軽減されました (これについては後で詳しく説明します)。 

*パラメータ化された Relu* (*PreLU*) 関数 :cite:`He.Zhang.Ren.ea.2015` など、ReLU 関数には多くのバリアントがあることに注意してください。このバリエーションにより ReLU に線形項が追加されるため、引数が負の場合でも、一部の情報は引き続き通過します。 

$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$

### シグモイド関数

[***シグモイド関数* は入力を変換します**] は $\mathbb{R}$ の範囲にある値です (**区間 (0, 1) にある出力へ**) そのため、シグモイドはしばしば「*squashing function*」と呼ばれます。この関数は範囲 (-inf, inf) の入力を (0, 1) の範囲の値に押しつぶします: 

$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$

初期のニューラルネットワークでは、科学者は*発火*または*発火しない*生物学的ニューロンのモデル化に興味を持っていました。したがって、この分野のパイオニアは、人工ニューロンの発明者であるマカロックとピッツにまでさかのぼり、しきい値測定ユニットに焦点を合わせました。しきい値処理のアクティブ化は、入力があるしきい値を下回る場合は値 0 を、入力がしきい値を上回った場合は値 1 を取ります。 

勾配ベースの学習に注目が移ったとき、シグモイド関数はしきい値単位に対する滑らかで微分可能な近似であるため、自然な選択でした。シグモイドは、出力をバイナリ分類問題の確率として解釈する場合 (シグモイドはソフトマックスの特殊なケースと考えることができます)、出力ユニットの活性化関数として広く使用されています。しかし、シグモイドは、隠れ層でのほとんどの使用のために、より単純でトレーニングが容易なReLUに置き換えられています。リカレントニューラルネットワークに関する後の章では、シグモイドユニットを活用して時間の経過に伴う情報の流れを制御するアーキテクチャについて説明します。 

以下に、シグモイド関数をプロットします。入力が 0 に近い場合、シグモイド関数は線形変換に近づくことに注意してください。

```{.python .input}
with autograd.record():
    y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

シグモイド関数の導関数は次の方程式で求められます。 

$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

シグモイド関数の導関数を以下にプロットします。入力が 0 の場合、シグモイド関数の導関数は最大 0.25 に達することに注意してください。入力が 0 からどちらかの方向に発散するにつれて、微分は 0 に近づきます。

```{.python .input}
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',
         figsize=(5, 2.5))
```

### タン関数

シグモイド関数と同様に、[**tanh (双曲線正接) 関数も入力をスカッシュ**] し、区間 (**-1 と 1 の間**) の要素に変換します。 

$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$

tanh 関数を以下にプロットします。入力が 0 に近づくにつれて、関数 tanh は線形変換に近づくことに注意してください。関数の形状はシグモイド関数の形状と似ていますが、tanh 関数は座標系の原点を中心に点対称性を示します。

```{.python .input}
with autograd.record():
    y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
y = tf.nn.tanh(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

tanh 関数の導関数は次のようになります。 

$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

tanh 関数の導関数を以下にプロットします。入力が 0 に近づくにつれて、関数 tanh の微分は最大値の 1 に近づきます。シグモイド関数で見たように、入力が 0 からいずれかの方向に移動すると、tanh 関数の導関数は0に近づきます。

```{.python .input}
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

```{.python .input}
#@tab pytorch
# Clear out previous gradients.
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

```{.python .input}
#@tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.tanh(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',
         figsize=(5, 2.5))
```

要約すると、表現力豊かな多層ニューラルネットワークアーキテクチャを構築するために非線形性を組み込む方法がわかりました。補足として、あなたの知識はすでに1990年頃に開業医と同様のツールキットを指揮しています。強力なオープンソースのディープラーニングフレームワークを活用して、わずか数行のコードでモデルを迅速に構築できるため、1990 年代に作業する誰よりも有利な点もあります。これまで、これらのネットワークのトレーニングでは、研究者は数千行の C と Fortran をコーディングする必要がありました。 

## [概要

* MLP は、出力層と入力層の間に 1 つまたは複数の完全に接続された隠れ層を追加し、活性化関数によって隠れ層の出力を変換します。
* 一般的に使用されるアクティベーション関数には、ReLU 関数、シグモイド関数、tanh 関数があります。

## 演習

1. preLU アクティベーション関数の微分を計算します。
1. ReLU (または preLU) のみを使用する MLP が連続区分的線形関数を構成することを示します。
1. $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$を見せて。
1. 一度に 1 つのミニバッチに適用される非線形性があると仮定します。これによってどのような問題が発生すると予想されますか？

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/90)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/91)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/226)
:end_tab:
