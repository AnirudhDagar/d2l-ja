# 順伝搬、逆方向伝播、計算グラフ
:label:`sec_backprop`

これまで、ミニバッチ確率的勾配降下法を使用してモデルをトレーニングしました。しかし、アルゴリズムを実装したときは、モデルを介した*前方伝播*に関わる計算についてのみ懸念していました。勾配を計算するときは、ディープラーニングフレームワークが提供するバックプロパゲーション関数を呼び出しました。 

勾配の自動計算 (自動微分) により、深層学習アルゴリズムの実装が大幅に簡素化されます。自動微分の前は、複雑なモデルを少しでも変更しても、複雑な微分を手作業で再計算する必要がありました。驚くべきことに、学術論文は更新規則を導出するために多数のページを割り当てる必要がありました。興味深い部分に焦点を当てるためには、引き続き自動微分に頼る必要がありますが、ディープラーニングの浅い理解を超えたい場合は、これらの勾配が内部でどのように計算されるかを知っておく必要があります。 

このセクションでは、*逆方向伝播* (より一般的には*backpropagation*) の詳細を掘り下げます。テクニックとその実装に関する洞察を伝えるために、基本的な数学と計算グラフを利用しています。まず、重量減衰 ($L_2$ 正則化) をもつ一隠れ層MLPに焦点をあてて解説します。 

## フォワード伝播

*フォワード伝播* (または*フォワードパス*) とは、計算と保存のことです。
入力層から出力層への順に、ニューラルネットワークの中間変数 (出力を含む) を順に示します。ここでは、隠れ層が 1 つあるニューラルネットワークの仕組みを順を追って説明します。これは退屈に思えるかもしれませんが、ファンクの巨匠ジェームズ・ブラウンの永遠の言葉では、「ボスになるための費用を払う」必要があります。 

簡単にするために、入力例が $\mathbf{x}\in \mathbb{R}^d$ で、隠れ層にバイアス項が含まれていないと仮定します。ここで、中間変数は次のようになります。 

$$\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},$$

$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$ は非表示レイヤの重みパラメータです。アクティベーション関数 $\phi$ を介して中間変数 $\mathbf{z}\in \mathbb{R}^h$ を実行すると、長さ $h$ の隠れたアクティベーションベクトルが得られます。 

$$\mathbf{h}= \phi (\mathbf{z}).$$

隠し変数 $\mathbf{h}$ も中間変数です。出力層のパラメーターの重みが $\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$ であると仮定すると、長さが $q$ のベクトルをもつ出力層変数を取得できます。 

$$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.$$

損失関数が $l$ で、ラベルの例が $y$ であると仮定すると、1 つのデータ例に対する損失項を計算できます。 

$$L = l(\mathbf{o}, y).$$

$L_2$ 正則化の定義によると、ハイパーパラメーター $\lambda$ が与えられた場合、正則化項は次のようになります。 

$$s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right),$$
:eqlabel:`eq_forward-s`

ここで、行列のフロベニウスノルムは、行列をベクトルに平坦化した後に適用された $L_2$ ノルムです。最後に、与えられたデータ例でのモデルの正則化損失は次のようになります。 

$$J = L + s.$$

以下の説明では $J$ を*目的関数* と呼びます。 

## フォワード伝搬の計算グラフ

*計算グラフ* をプロットすると、計算における演算子と変数の依存関係を視覚化するのに役立ちます。:numref:`fig_forward` には、前述の単純なネットワークに関連するグラフが含まれ、四角は変数を表し、円は演算子を表します。左下隅が入力を表し、右上隅が出力を表します。矢印の方向 (データフローを示す) は、主に右向きと上向きであることに注意してください。 

![Computational graph of forward propagation.](../img/forward.svg)
:label:`fig_forward`

## バックプロパゲーション

*バックプロパゲーション* とは、計算の方法を指します。
ニューラルネットワークのパラメーターの勾配。つまり、この方法は、微積分の*チェーンルール*に従って、出力層から入力層まで逆の順序でネットワークをトラバースします。このアルゴリズムは、一部のパラメーターに関する勾配を計算するときに必要な中間変数 (偏微分) を保存します。関数 $\mathsf{Y}=f(\mathsf{X})$ と $\mathsf{Z}=g(\mathsf{Y})$ があり、入力と出力 $\mathsf{X}, \mathsf{Y}, \mathsf{Z}$ が任意の形状のテンソルであると仮定します。連鎖則を使うことで、$\mathsf{X}$ に対する $\mathsf{Z}$ の微分を次のように計算できます。 

$$\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).$$

ここでは $\text{prod}$ 演算子を使用して、転置や入力位置の入れ替えなどの必要な演算を実行した後に、その引数を乗算します。ベクトルの場合、これは単純に行列-行列の乗算です。高次元のテンソルには、対応するテンソルを使用します。演算子 $\text{prod}$ は表記法のオーバーヘッドをすべて隠します。 

計算グラフが :numref:`fig_forward` にある 1 つの隠れ層をもつ単純ネットワークのパラメーターは $\mathbf{W}^{(1)}$ と $\mathbf{W}^{(2)}$ であることを思い出してください。逆伝播の目的は、勾配 $\partial J/\partial \mathbf{W}^{(1)}$ と $\partial J/\partial \mathbf{W}^{(2)}$ を計算することです。これを実現するために、チェーンルールを適用し、各中間変数とパラメーターの勾配を計算します。計算グラフの結果から始めて、パラメーターに向かって作業する必要があるため、計算の順序は順伝播で実行される順序とは逆になります。最初のステップは、損失項 $L$ と正則化項 $s$ に関する目的関数 $J=L+s$ の勾配を計算することです。 

$$\frac{\partial J}{\partial L} = 1 \; \text{and} \; \frac{\partial J}{\partial s} = 1.$$

次に、チェーンルールに従って、出力層 $\mathbf{o}$ の変数に対する目的関数の勾配を計算します。 

$$
\frac{\partial J}{\partial \mathbf{o}}
= \text{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)
= \frac{\partial L}{\partial \mathbf{o}}
\in \mathbb{R}^q.
$$

次に、両方のパラメーターに関する正則化項の勾配を計算します。 

$$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}
\; \text{and} \;
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.$$

これで、出力レイヤーに最も近いモデルパラメーターの勾配 $\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$ を計算できるようになりました。連鎖規則を使用すると、次の結果が得られます。 

$$\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.$$
:eqlabel:`eq_backprop-J-h`

$\mathbf{W}^{(1)}$ に関する勾配を得るには、出力層に沿って隠れ層への逆伝播を続ける必要があります。隠れ層の出力 $\partial J/\partial \mathbf{h} \in \mathbb{R}^h$ に対する勾配は次の式で与えられます。 

$$
\frac{\partial J}{\partial \mathbf{h}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{h}}\right)
= {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}.
$$

活性化関数 $\phi$ は要素単位で適用されるため、中間変数 $\mathbf{z}$ の勾配 $\partial J/\partial \mathbf{z} \in \mathbb{R}^h$ を計算するには、$\odot$ で表される要素単位の乗算演算子を使用する必要があります。 

$$
\frac{\partial J}{\partial \mathbf{z}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{h}}, \frac{\partial \mathbf{h}}{\partial \mathbf{z}}\right)
= \frac{\partial J}{\partial \mathbf{h}} \odot \phi'\left(\mathbf{z}\right).
$$

最後に、入力層に最も近いモデルパラメーターの勾配 $\partial J/\partial \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$ を取得できます。連鎖規則によれば、 

$$
\frac{\partial J}{\partial \mathbf{W}^{(1)}}
= \text{prod}\left(\frac{\partial J}{\partial \mathbf{z}}, \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(1)}}\right)
= \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.
$$

## ニューラルネットワークの学習

ニューラルネットワークに学習させる場合、順伝播と逆伝播は互いに依存します。特に、順伝播では、計算グラフを依存関係の方向にトラバースし、そのパス上のすべての変数を計算します。これらは逆伝播に使用され、グラフ上の計算順序が逆になります。 

前述の単純なネットワークを例に挙げて説明します。一方では、順伝播中の正則化項 :eqref:`eq_forward-s` の計算は、モデルパラメーター $\mathbf{W}^{(1)}$ と $\mathbf{W}^{(2)}$ の現在の値に依存します。これらは、最新のイテレーションのバックプロパゲーションに従って、最適化アルゴリズムによって与えられます。一方、バックプロパゲーション中のパラメーター :eqref:`eq_backprop-J-h` の勾配計算は、フォワードプロパゲーションによって与えられる隠れ変数 $\mathbf{h}$ の現在の値に依存します。 

したがって、ニューラルネットワークの学習時には、モデルパラメーターの初期化後に、順伝播と逆伝播を交互に行い、バックプロパゲーションによって与えられた勾配を使用してモデルパラメーターを更新します。バックプロパゲーションでは、計算の重複を避けるため、前方伝播の格納された中間値が再利用されることに注意してください。その結果の 1 つは、逆伝播が完了するまで中間値を保持する必要があることです。これは、トレーニングが単純な予測よりもはるかに多くのメモリを必要とする理由の 1 つでもあります。また、このような中間値のサイズは、ネットワーク層の数とバッチサイズにほぼ比例します。したがって、より大きなバッチサイズを使用してより深いネットワークに学習させると、「メモリ不足*」エラーが発生しやすくなります。 

## [概要

* フォワードプロパゲーションは、ニューラルネットワークによって定義される計算グラフ内で中間変数を順次計算して保存します。入力層から出力層へと進みます。
* バックプロパゲーションは、ニューラルネットワーク内の中間変数とパラメーターの勾配を逆の順序で順次計算して保存します。
* ディープラーニングモデルに学習させる場合、順伝播と逆伝播は相互に依存しています。
* トレーニングには予測よりもはるかに多くのメモリが必要です。

## 演習

1. あるスカラー関数 $f$ に対する入力 $\mathbf{X}$ は $n \times m$ 行列であると仮定します。$\mathbf{X}$ に対する $f$ の勾配の次元はどれくらいですか？
1. このセクションで説明するモデルの隠れ層にバイアスを追加します (正則化項にバイアスを含める必要はありません)。
    1. 対応する計算グラフを描画します。
    1. 順伝播方程式と逆方向伝播方程式を導出する。
1. このセクションで説明するモデルで、学習と予測のためのメモリフットプリントを計算します。
1. 2 次導関数を計算すると仮定します。計算グラフはどうなりますか？計算にはどれくらい時間がかかると思いますか。
1. 計算グラフが GPU に対して大きすぎると仮定します。
    1. 複数の GPU にパーティション分割できますか？
    1. 小さいミニバッチでのトレーニングに勝るメリットとデメリットは何ですか？

[Discussions](https://discuss.d2l.ai/t/102)
