# 数値の安定性と初期化
:label:`sec_numerical_stability`

これまで、実装したすべてのモデルでは、あらかじめ指定された分布に従ってパラメーターを初期化する必要がありました。これまで、私たちは初期化スキームを当たり前のことと考えていましたが、これらの選択がどのように行われるかの詳細については見落としていました。これらの選択は特に重要ではないという印象を受けたかもしれません。逆に、初期化方式の選択はニューラルネットワークの学習において重要な役割を果たし、数値の安定性を維持する上で非常に重要になる可能性がある。さらに、これらの選択は、非線形活性化関数の選択と興味深い方法で結びつけることができます。どの関数を選択し、どのようにパラメーターを初期化するかによって、最適化アルゴリズムが収束する速度が決まります。ここでの選択が悪いと、トレーニング中に爆発したり消えたりする勾配に遭遇する可能性があります。このセクションでは、これらのトピックをより詳細に掘り下げ、ディープラーニングのキャリアを通じて役立つと思われる有用なヒューリスティックについて説明します。 

## 消失するグラデーションと爆発するグラデーション

$L$ 層、入力 $\mathbf{x}$、出力 $\mathbf{o}$ をもつディープネットワークについて考えてみます。隠れ変数が $\mathbf{h}^{(l)}$ ($\mathbf{h}^{(0)} = \mathbf{x}$) である重み $\mathbf{W}^{(l)}$ でパラメーター化された変換 $f_l$ によって定義される各レイヤー $l$ では、ネットワークは次のように表すことができます。 

$$\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ and thus } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).$$

すべての隠れ変数と入力がベクトルの場合、$\mathbf{W}^{(l)}$ の任意のパラメーターセットに対する $\mathbf{o}$ の勾配を次のように記述できます。 

$$\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.$$

つまり、この勾配は $L-l$ 行列 $\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$ と勾配ベクトル $\mathbf{v}^{(l)}$ の積になります。したがって、あまりにも多くの確率を掛け合わせるとしばしば発生する数値アンダーフローの問題と同じ影響を受けやすくなります。確率を扱う場合、一般的なトリックは対数空間に切り替える、つまり圧力を仮数から数値表現の指数にシフトさせることです。残念ながら、上記の問題はもっと深刻です。最初は、行列 $\mathbf{M}^{(l)}$ は多種多様な固有値をもつ可能性があります。それらは小さい場合と大きい場合があり、製品は*非常に大きい*または*非常に小さい場合があります。 

不安定な勾配によってもたらされるリスクは、数値表現を超えています。予測不可能な大きさの勾配も、最適化アルゴリズムの安定性を脅かしています。(i) 過度に大きくてモデルが破壊される (*爆発する勾配* 問題)、(ii) 過度に小さい (*消失する勾配* 問題)、更新のたびにパラメータがほとんど移動しないため、学習が不可能になるパラメータの更新に直面している可能性があります。 

### (**消失するグラデーション**)

消失勾配の問題を引き起こす原因の 1 つは、各層の線形演算の後に追加される活性化関数 $\sigma$ の選択です。従来、シグモイド関数 $1/(1 + \exp(-x))$ (:numref:`sec_mlp` で導入) はしきい値処理関数に似ているため人気がありました。初期の人工ニューラルネットワークは生物学的ニューラルネットワークに触発されていたため、（生体ニューロンのように）*完全に*発火するか、まったく*発火しないニューロンのアイデアは魅力的に思えました。シグモイドが勾配の消失を引き起こす原因を詳しく見てみましょう。

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()

x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.sigmoid(x)
y.backward()

d2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

x = tf.Variable(tf.range(-8.0, 8.0, 0.1))
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

ご覧のとおり (**シグモイドの勾配は、入力が大きい場合と小さい場合の両方で消滅します**)。さらに、多くの層を逆伝播する場合、多くのシグモイドへの入力がゼロに近いゴルディロックゾーンにいない限り、積全体の勾配が消滅する可能性があります。ネットワークに多くのレイヤーがある場合、注意しない限り、あるレイヤーでグラデーションが途切れる可能性があります。実際、この問題はディープネットワークトレーニングを悩ませていました。その結果、より安定している (しかし神経的にもっともらしくない) RELUは、開業医にとってデフォルトの選択肢として浮上している。 

### [**分解するグラデーション**]

グラデーションが爆発すると、逆の問題も同様に厄介になる可能性があります。これをもう少しわかりやすく説明するために、100 個のガウス乱数行列を描き、それに初期行列を掛けます。選択したスケール (分散 $\sigma^2=1$ の選択) では、行列積が爆発します。ディープネットワークの初期化が原因でこれが発生した場合、勾配降下オプティマイザが収束する可能性はありません。

```{.python .input}
M = np.random.normal(size=(4, 4))
print('a single matrix', M)
for i in range(100):
    M = np.dot(M, np.random.normal(size=(4, 4)))

print('after multiplying 100 matrices', M)
```

```{.python .input}
#@tab pytorch
M = torch.normal(0, 1, size=(4,4))
print('a single matrix \n',M)
for i in range(100):
    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))

print('after multiplying 100 matrices\n', M)
```

```{.python .input}
#@tab tensorflow
M = tf.random.normal((4, 4))
print('a single matrix \n', M)
for i in range(100):
    M = tf.matmul(M, tf.random.normal((4, 4)))

print('after multiplying 100 matrices\n', M.numpy())
```

### シンメトリを破る

ニューラルネットワークの設計におけるもう 1 つの問題は、パラメータ化に内在する対称性です。1 つの隠れ層と 2 つのユニットをもつ単純な MLP があると仮定します。この場合、第 1 層の重み $\mathbf{W}^{(1)}$ を並び替え、同様に出力層の重みを置換して同じ関数を得ることができます。第1隠れユニットと第2隠しユニットを区別する特別なものはありません。言い換えれば、各層の隠れユニット間に順列対称性があるということです。 

これは単なる理論上の迷惑ではありません。2 つの隠れユニットを持つ、前述の 1 つの隠れ層 MLP について考えてみます。説明のために、出力層が 2 つの非表示単位を 1 つの出力単位だけに変換するとします。ある定数 $c$ に対して、隠れ層のすべてのパラメータを $\mathbf{W}^{(1)} = c$ として初期化するとどうなるか想像してみてください。この場合、フォワード伝播中に、隠れユニットが同じ入力とパラメータを受け取り、同じアクティベーションを生成して出力ユニットに供給します。バックプロパゲーション中、パラメーター $\mathbf{W}^{(1)}$ に関して出力単位を微分すると、要素がすべて同じ値を取る勾配が得られます。したがって、勾配ベースの反復 (ミニバッチ確率的勾配降下法など) の後も、$\mathbf{W}^{(1)}$ のすべての要素は同じ値を取ります。このような反復は、それ自体では*対称性を破る*ことはなく、ネットワークの表現力は決して実現できないかもしれません。非表示レイヤーは、ユニットが 1 つしかないかのように動作します。ミニバッチ確率的勾配降下法はこの対称性を破ることはありませんが、ドロップアウト正則化は壊れることに注意してください。 

## パラメーターの初期化

上記で提起された問題に対処する (少なくとも軽減する) 方法の 1 つは、慎重に初期化することです。最適化時の注意と適切な正則化により、安定性をさらに高めることができます。 

### 既定の初期化

前のセクション、例えば :numref:`sec_linear_concise` では、正規分布を使用して重みの値を初期化しました。初期化方法を指定しない場合、フレームワークはデフォルトのランダム初期化方法を使用します。これは、中程度の問題サイズに対しては実際にうまく機能することがよくあります。 

### ザビエルの初期化
:label:`subsec_xavier`

全結合層に対する出力 (隠れ変数など) $o_{i}$ のスケール分布を見てみましょう。
*非線形性なし*。
この層の $n_\mathrm{in}$ の入力 $x_j$ とそれに関連する重み $w_{ij}$ の場合、出力は次の式で与えられます。 

$$o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.$$

重み $w_{ij}$ は、すべて同じ分布から独立して描画されます。さらに、この分布の平均がゼロで分散 $\sigma^2$ であると仮定します。これは、分布がガウス分布でなければならないという意味ではなく、単に平均と分散が存在する必要があるということだけであることに注意してください。ここでは、層 $x_j$ への入力もゼロの平均と分散 $\gamma^2$ をもち、$w_{ij}$ から独立していて互いに独立していると仮定します。この場合、$o_i$ の平均と分散は次のように計算できます。 

$$
\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$

分散を固定する 1 つの方法は $n_\mathrm{in} \sigma^2 = 1$ を設定することです。ここでバックプロパゲーションについて考えてみましょう。そこでは、出力に近いレイヤーから勾配が伝播されるにもかかわらず、同様の問題に直面します。順伝播と同じ推論を使用すると、$n_\mathrm{out} \sigma^2 = 1$ ($n_\mathrm{out}$ はこの層の出力数) でない限り、勾配の分散が爆発する可能性があることがわかります。これはジレンマに陥ります。両方の条件を同時に満たすことはできません。代わりに、私たちは単に以下を満たそうとします。 

$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ or equivalently }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$

これが、今や標準的で実用的に有益な*Xavier initialization* の根底にある理由であり、その作成者 :cite:`Glorot.Bengio.2010` の最初の作者にちなんで名付けられました。通常、Xavier 初期化では、平均がゼロで分散 $\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$ をもつガウス分布から重みがサンプリングされます。また、一様分布から重みをサンプリングするときに、ザビエルの直感を適応させて分散を選択することもできます。一様分布 $U(-a, a)$ には分散 $\frac{a^2}{3}$ があることに注意してください。$\frac{a^2}{3}$ を $\sigma^2$ の状態に差し込むと、以下のように初期化するよう提案されます。 

$$U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).$$

上記の数学的推論で非線形性が存在しないという仮定は、ニューラルネットワークでは容易に破られる可能性がありますが、実際にはXavier初期化法がうまく機能することがわかりました。 

### 超えて

上記の推論は、パラメータ初期化に対する現代的なアプローチの表面をほとんど傷つけません。ディープラーニングフレームワークでは、十数種類以上のヒューリスティックが実装されることがよくあります。さらに、パラメーターの初期化は、ディープラーニングの基礎研究のホットエリアであり続けています。その中には、タイド (共有) パラメータ、超解像、シーケンスモデル、およびその他の状況に特化したヒューリスティックがあります。たとえば、Xiao らは、慎重に設計された初期化メソッド :cite:`Xiao.Bahri.Sohl-Dickstein.ea.2018` を使用して、アーキテクチャ上のトリックなしに 10000 層のニューラルネットワークをトレーニングできる可能性を示しました。 

トピックに興味がある場合は、このモジュールの内容を深く掘り下げて、各ヒューリスティックを提案および分析した論文を読み、そのトピックに関する最新の出版物を調べることをお勧めします。たぶん、あなたはつまずいたり、巧妙なアイデアを発明したり、ディープラーニングフレームワークの実装に貢献したりするでしょう。 

## [概要

* グラデーションの消失と爆発は、ディープネットワークではよくある問題です。勾配とパラメーターを適切に制御するには、パラメーターの初期化に細心の注意を払う必要があります。
* 初期勾配が大きすぎたり小さすぎたりしないようにするには、初期化ヒューリスティックが必要です。
* ReLU アクティベーション関数は、消失する勾配の問題を軽減します。これにより、コンバージェンスが加速します。
* 最適化前に対称性が崩れるようにするには、ランダム初期化が重要です。
* ザビエルの初期化では、各層について、出力の分散は入力数の影響を受けず、勾配の分散は出力数の影響を受けないことが示唆されています。

## 演習

1. ニューラルネットワークが、MLP の層における順列対称性以外に、破断を必要とする対称性を示す可能性のある他のケースを設計できますか？
1. 線形回帰またはソフトマックス回帰のすべての重みパラメータを同じ値に初期化できますか？
1. 2 つの行列の積の固有値で解析限界を調べます。これは、グラデーションが適切に調整されていることを確認することについて何を教えてくれますか？
1. いくつかの用語が発散していることがわかっている場合、事後にこれを修正できますか？インスピレーションを得るには、レイヤーワイズアダプティブレートスケーリングに関する論文を見てください :cite:`You.Gitman.Ginsburg.2017`。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/103)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/104)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/235)
:end_tab:
