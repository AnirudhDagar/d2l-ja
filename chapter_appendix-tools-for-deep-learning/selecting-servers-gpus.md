# サーバと GPU の選択
:label:`sec_buy_gpu`

通常、ディープラーニングの学習には大量の計算が必要です。現在、GPU はディープラーニングで最も費用対効果の高いハードウェアアクセラレータです。特に、CPU と比較すると、GPU は安価でパフォーマンスが高く、多くの場合 1 桁以上になります。さらに、1 台のサーバーで複数の GPU をサポートでき、ハイエンドサーバーでは最大 8 個の GPU をサポートできます。熱、冷却、電力の要件はオフィスビルでサポートできる範囲を超えて急速に高まるため、エンジニアリングワークステーションでは最大 4 つの GPU が一般的です。大規模なデプロイメントでは、Amazon の [P3](https://aws.amazon.com/ec2/instance-types/p3/) インスタンスや [G4](https://aws.amazon.com/blogs/aws/in-the-works-ec2-instances-g4-with-nvidia-t4-gpus/) インスタンスなどのクラウドコンピューティングの方がはるかに実用的なソリューションです。 

## サーバを選択する

計算の多くは GPU で行われるため、通常はスレッド数の多いハイエンド CPU を購入する必要はありません。とはいえ、Python のグローバルインタープリタロック (GIL) により、4 ～ 8 個の GPU がある状況では、CPU のシングルスレッドのパフォーマンスが問題になる可能性があります。すべて同じことは、コア数は少ないがクロック周波数が高いCPUの方が経済的な選択肢になる可能性があることを示唆しています。たとえば、6 コア 4 GHz と 8 コア 3.5 GHz CPU のどちらかを選択する場合、総速度は低いものの、前者の方がはるかに適しています。重要な考慮事項として、GPU は大量の電力を使用するため、大量の熱を放散します。これには、非常に良好な冷却と、GPU を使用するのに十分な大きさのシャーシが必要です。可能であれば、次のガイドラインに従ってください。 

1. **電源装置**。GPU は大量の電力を消費します。デバイスあたり最大 350 W の予算 (効率的なコードでは大量のエネルギーが消費されるため、通常の需要ではなく、グラフィックスカードの*ピーク時の需要*を確認してください)。電源が要求に達していないと、システムが不安定になることがあります。
1. **シャーシサイズ**。GPU は大きく、補助電源コネクタには余分なスペースが必要になることがよくあります。また、シャーシが大きいほど冷却が容易です。
1. **GPU 冷却**。GPU の数が多い場合は、水冷に投資することをお勧めします。また、ファン数が少なくても*リファレンスデザイン*を目指してください。デバイス間の空気取り入れが可能な薄さです。マルチファン GPU を購入した場合、複数の GPU を取り付けるときに十分な空気を得るには厚すぎて、サーマルスロットリングが発生する可能性があります。
1. ** PCIe スロット**。GPU との間でデータを移動する (および GPU 間でデータを交換する) には、大量の帯域幅が必要です。16 レーンの PCIe 3.0 スロットをお勧めします。複数の GPU をマウントする場合は、マザーボードの説明をよく読んで、複数の GPU を同時に使用しても 16 倍の帯域幅が使用可能であること、および追加スロット用に PCIe 2.0 ではなく PCIe 3.0 が使用されていることを確認してください。マザーボードによっては、複数の GPU を取り付けた状態で 8 倍または 4 倍の帯域幅にダウングレードするものもあります。これは、CPU が提供する PCIe レーンの数に一部起因しています。

つまり、ディープラーニングサーバーを構築するための推奨事項をいくつか紹介します。 

* **初心者**。低消費電力のローエンド GPU を購入する (ディープラーニングに適した安価なゲーミング GPU は 150 ～ 200 W を使用)。運が良ければ、現在のコンピューターでサポートされます。
* **1 GPU**。4コアのローエンドCPUで十分で、ほとんどのマザーボードで十分です。32 GB 以上の DRAM を目標とし、ローカルデータアクセス用の SSD に投資します。600W の電源装置で十分です。たくさんのファンがいるGPUを購入する。
* ** 2 GPU **。コア数が 4 ～ 6 のローエンド CPU で十分です。64 GB の DRAM を目指して、SSD に投資します。2 台のハイエンド GPU には 1000 W のオーダーが必要です。メインボードに関しては、PCIe 3.0 x16 スロットが 2 つあることを確認してください。可能であれば、PCIe 3.0 x16 スロットの間に 2 つの空きスペース (60 mm 間隔) があるメインボードを用意して、空気を余分に確保します。この場合は、ファンの多いGPUを2つ購入してください。
* ** 4 GPU **。シングルスレッドの速度が比較的速い (つまり、クロック周波数が高い) CPU を購入するようにしてください。AMD Threadripper など、PCIe レーンの数が多い CPU が必要になる場合があります。PCIe 3.0 x16 スロットを 4 つ使用するには、比較的高価なメインボードが必要になる可能性があります。PCIe レーンをマルチプレクシングするには PLX が必要なためです。狭いリファレンスデザインの GPU を購入し、GPU 間に空気を入れます。1600 ～ 2000 W の電源装置が必要ですが、オフィスのコンセントではサポートされない場合があります。このサーバーはおそらく*大音量でホット*で動作します。机の下には置きたくない。128 GB の DRAM が推奨されます。ローカルストレージ用の SSD (1-2 TB NVMe) と、データを保存するための RAID 構成の一連のハードディスクを入手してください。
* **8 GPU **。複数の冗長電源装置を備えた専用のマルチ GPU サーバシャーシを購入する必要がある (電源装置あたり 1600 W の場合は 2+1 など)。これには、デュアルソケットサーバー CPU、256 GB ECC DRAM、高速ネットワークカード (10 GBE 推奨) が必要です。また、サーバーが GPU の*物理フォームファクター* をサポートしているかどうかを確認する必要があります。コンシューマーとサーバーの GPU では、エアーフローと配線の配置が大きく異なります (RTX 2080 と Tesla V100 など)。これは、電源ケーブルのスペースが不十分だったり、適切なワイヤーハーネスがないために (共著者の1人が痛々しく発見したように)、コンシューマーGPUをサーバーに取り付けることができない可能性があることを意味します。

## GPU を選択する

現在、AMDとNVIDIAは専用GPUの2つの主要メーカーです。NVIDIA はディープラーニング分野に初めて参入し、CUDA を介してディープラーニングフレームワークのサポートを強化しています。したがって、ほとんどの購入者はNVIDIA GPUを選択します。 

NVIDIA は、個人ユーザー (GTX や RTX シリーズなど) とエンタープライズユーザー (Tesla シリーズ) を対象とした 2 種類の GPU を提供しています。この 2 種類の GPU は、同等の処理能力を提供します。ただし、エンタープライズユーザー GPU は一般に (パッシブ) 強制冷却、より多くのメモリ、および ECC (エラー修正) メモリを使用します。これらの GPU はデータセンターに適しており、通常はコンシューマ GPU の 10 倍のコストがかかります。 

100台以上のサーバーを持つ大企業の場合は、NVIDIA Teslaシリーズを検討するか、クラウドでGPUサーバーを使用することをお勧めします。ラボや 10 台以上のサーバーを持つ中小企業では、NVIDIA RTX シリーズが最も費用対効果が高いと思われます。4 ～ 8 個の GPU を効率的に保持する Supermicro または Asus シャーシを搭載した構成済みサーバーを購入できます。 

GPU ベンダーは通常、2017 年にリリースされた GTX 1000 (Pascal) シリーズや 2019 年にリリースされた RTX 2000 (Turing) シリーズなど、1 ～ 2 年ごとに新世代をリリースします。各シリーズには、さまざまなパフォーマンスレベルを提供する複数の異なるモデルがあります。GPU のパフォーマンスは、主に次の 3 つのパラメータを組み合わせたものです。 

1. **計算能力**。通常、32ビット浮動小数点演算能力が求められます。16ビット浮動小数点トレーニング (FP16) も主流になりつつあります。予測のみに関心がある場合は、8 ビット整数を使用することもできます。最新世代の Turing GPU は、4 ビットアクセラレーションを提供します。残念ながら現在、低精度のネットワークを学習させるアルゴリズムはまだ普及していません。
1. **メモリサイズ**。モデルが大きくなったり、学習中に使用されるバッチが大きくなったりすると、より多くの GPU メモリが必要になります。HBM2 (高帯域幅メモリ) と GDDR6 (グラフィックス DDR) メモリを確認します。HBM2 は高速ですが、はるかに高価です。
1. **メモリ帯域幅**。十分なメモリ帯域幅がある場合にのみ、計算処理能力を最大限に引き出すことができます。GDDR6 を使用する場合は、ワイドメモリバスを探してください。

ほとんどのユーザーにとって、計算能力を見れば十分です。多くの GPU では異なるタイプのアクセラレーションが提供されることに注意してください。たとえば、NVIDIA の TensorCore は、オペレータのサブセットを 5 倍高速化します。ライブラリがこれをサポートしていることを確認してください。GPUメモリは4 GB以上である必要があります（8 GBの方がはるかに優れています）。GUI の表示にも GPU を使用しないようにしてください (代わりに組み込みのグラフィックを使用してください)。避けられない場合は、安全のために2 GBのRAMを追加してください。 

:numref:`fig_flopsvsprice` は、GTX 900、GTX 1000、RTX 2000 の各シリーズモデルの 32 ビット浮動小数点演算能力と価格を比較しています。価格はウィキペディアに掲載されている推奨価格です。 

![Floating-point compute power and price comparison. ](../img/flopsvsprice.svg)
:label:`fig_flopsvsprice`

私たちは多くのことを見ることができます。 

1. 各シリーズでは、価格とパフォーマンスはほぼ比例します。Titan モデルは、GPU メモリを大量に消費するというメリットから、かなりのプレミアムを要します。ただし、980 Tiと1080 Tiを比較するとわかるように、新しいモデルの方が費用対効果が高くなります。RTX 2000シリーズの価格はあまり改善していないようです。しかし、これははるかに優れた低精度性能 (FP16、INT8、INT4) を提供するためです。
2. GTX 1000シリーズの性能対コスト比は、900シリーズの約2倍です。
3. RTX 2000シリーズでは、価格は価格の*アフィン*関数です。

![Floating-point compute power and energy consumption. ](../img/wattvsprice.svg)
:label:`fig_wattvsprice`

:numref:`fig_wattvsprice` は、エネルギー消費量が計算量に応じてほぼ直線的に増大する様子を示しています。第二に、後の世代はより効率的です。これはRTX 2000シリーズに対応したグラフと矛盾しているようだ。しかし、これはTensorCoresが不釣り合いに多くのエネルギーを引き出す結果です。 

## [概要

* サーバを構築する際は、電力、PCIe バスレーン、CPU シングルスレッド速度、冷却に気をつけてください。
* 可能であれば、最新の GPU 世代を購入する必要があります。
* 大規模な導入にはクラウドを使用します。
* 高密度サーバーは、すべての GPU と互換性がない場合があります。購入前に機械仕様と冷却仕様を確認してください。
* 高効率のためにはFP16以下の精度を使用してください。

[Discussions](https://discuss.d2l.ai/t/425)
