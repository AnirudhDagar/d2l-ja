# ハードウェア
:label:`sec_hardware`

優れたパフォーマンスを発揮するシステムを構築するには、問題の統計的側面を把握するためのアルゴリズムとモデルを十分に理解する必要があります。同時に、基盤となるハードウェアについて少なくともある程度の知識を持つことも不可欠です。現在のセクションは、ハードウェアとシステムの設計に関する適切なコースに代わるものではありません。その代わり、一部のアルゴリズムが他のアルゴリズムよりも効率的である理由と、優れたスループットを達成する方法を理解するための出発点となる可能性があります。優れた設計は容易に桁違いを生むことがあり、ひいては、ネットワークをトレーニングできること (たとえば、1 週間以内) とまったくトレーニングしない (3 か月で期限を過ぎる) ことの違いを生む可能性があります。まず、コンピューターについて見ていきます。次に、CPU と GPU をより注意深く見るためにズームインします。最後に、サーバーセンターまたはクラウドで複数のコンピューターがどのように接続されているかを確認するためにズームアウトします。  

![Latency Numbers that every programmer should know.](../img/latencynumbers.png)
:label:`fig_latencynumbers`

せっかちな読者は:numref:`fig_latencynumbers`でうまくやれるかもしれません。コリン・スコットの [インタラクティブポスト](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html) から引用され、過去10年間の進歩の概観がよくわかる。元の数字はジェフ・ディーンの[Stanford talk from 2010](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf)によるものです。以下の説明では、これらの数値の理論的根拠の一部と、それらがアルゴリズムの設計にどのように導くことができるかを説明します。以下の議論は非常に高レベルで大雑把です。これは明らかに、適切なコースに代わるものではなく、統計モデラーが適切な設計決定を下すのに十分な情報を提供することを目的としています。コンピュータアーキテクチャの詳細な概要については、:cite:`Hennessy.Patterson.2011`、または [Arste Asanovic](http://inst.eecs.berkeley.edu/~cs152/sp19/) のような、このテーマに関する最近のコースを参照してください。 

## コンピューター

ほとんどのディープラーニングの研究者や実践者は、かなりの量のメモリ、計算、GPU などの何らかのアクセラレータ、あるいはその倍数を持つコンピューターにアクセスできます。コンピュータは、次の主要コンポーネントで構成されています。 

* 与えられたプログラムを実行できるプロセッサ (CPU とも呼ばれる) で、オペレーティングシステムやその他の多くの処理を実行でき、通常は 8 個以上のコアで構成されます。
* 重みベクトル、アクティベーション、トレーニングデータなど、計算結果を保存および取得するためのメモリ (RAM)。
* 1 Gbps から 100 Gbps までの速度のイーサネットネットワーク接続 (複数の場合もある)。ハイエンドサーバーでは、より高度な相互接続が可能です。
* システムを 1 つ以上の GPU に接続するための高速拡張バス (PCIe)。サーバには最大 8 つのアクセラレータがあり、多くの場合、高度なトポロジで接続されます。デスクトップシステムには、ユーザの予算と電源のサイズに応じて 1 つまたは 2 つのアクセラレータがあります。
* 多くの場合、PCIeバスを使用して接続された磁気ハードディスクドライブ、ソリッドステートドライブなどの耐久性のあるストレージ。これにより、トレーニングデータをシステムに効率的に転送し、必要に応じて中間チェックポイントを保存できます。

![Connectivity of components of a computer.](../img/mobo-symbol.svg)
:label:`fig_mobo-symbol`

:numref:`fig_mobo-symbol` が示すように、ほとんどのコンポーネント (ネットワーク、GPU、ストレージ) は PCIe バスを介して CPU に接続されています。CPU に直接接続される複数のレーンで構成されます。たとえば、AMDのThreadripper 3には64のPCIe 4.0レーンがあり、各レーンは両方向で16ギガビット/秒のデータ転送が可能です。メモリは CPU に直接接続され、合計帯域幅は最大 100 Gb/s です。 

コンピューターでコードを実行するときは、データをプロセッサ (CPU または GPU) にシャッフルし、計算を実行してから、その結果をプロセッサから RAM と耐久性のあるストレージに戻す必要があります。したがって、良好なパフォーマンスを得るためには、どのシステムも大きなボトルネックになることなく、これがシームレスに動作することを確認する必要があります。例えば、イメージを十分に速く読み込めなければ、プロセッサには何の作業も行われません。同様に、行列を CPU (または GPU) に十分速く移動できない場合、その処理要素は枯渇します。最後に、ネットワーク上で複数のコンピュータを同期させたい場合、後者は計算速度を低下させないはずです。1 つの選択肢は、通信と計算をインターリーブすることです。さまざまなコンポーネントについて詳しく見ていきましょう。 

## メモリー

最も基本的なメモリは、容易にアクセスできるようにする必要があるデータを格納するために使用されます。現在、CPU RAM は一般的に [DDR4](https://en.wikipedia.org/wiki/DDR4_SDRAM) 種類で、モジュールあたり 20 ～ 25 GB/s の帯域幅を提供します。各モジュールには 64 ビット幅のバスを搭載しています。通常、メモリモジュールのペアは複数チャネルに対応するために使用されます。CPU には 2 ～ 4 個のメモリチャネルがあります。つまり、4 つの 0 Gb/s から 100 Gb/s のピークメモリ帯域幅があります。多くの場合、チャネルごとに 2 つのバンクがあります。例えば、AMDのZen 3 Threadripperには8つのスロットがあります。 

これらの数字は印象的ですが、確かに、それらは物語の一部しか伝えていません。メモリから一部を読み取るときは、まずメモリモジュールに情報がある場所を伝える必要があります。つまり、まず*アドレス*をRAMに送信する必要があります。これが完了したら、単一の 64 ビットレコードのみを読み取るか、レコードの長いシーケンスを読み取るかを選択できます。後者を*burst read* と呼びます。一言で言えば、アドレスをメモリに送信して転送を設定するのにかかる時間は約100 ns（詳細は使用されるメモリチップの特定のタイミング係数によって異なります）、その後の転送には0.2 nsしかかかりません。つまり、最初の読み取りは、その後の読み取りの500倍のコストがかかります。1 秒間に最大 10,000,000 回のランダム読み取りを実行できることに注意してください。これは、可能な限りランダムなメモリアクセスを避け、代わりにバースト読み取り (および書き込み) を使用することを示唆しています。 

複数の*銀行*があることを考慮すると、問題は少し複雑になります。各バンクはメモリをほぼ独立して読み取ることができます。これは二つのことを意味します。一方では、ランダムリードの実効数は、メモリ全体に均等に分散されていれば、最大 4 倍になります。また、バースト読み取りも 4 倍高速であるため、ランダム読み取りを実行することは依然として悪い考えです。一方、メモリーは64ビット境界にアライメントされるため、すべてのデータ構造を同じ境界に揃えるのが良い考えです。コンパイラは、適切なフラグが設定されている場合、[automatically](https://en.wikipedia.org/wiki/Data_structure_alignment) とほぼ同じ処理を行います。好奇心旺盛な読者には、[Zeshan Chishti](http://web.cecs.pdx.edu/~zeshan/ece585_lec5.pdf) によるようなDRAMに関するレクチャーを復習することをお勧めします。 

GPU メモリは CPU よりも多くの処理要素を備えているため、さらに高い帯域幅要件が課せられます。概して、それらに対処するには2つの選択肢があります。1つ目は、メモリバスを大幅に広くすることです。たとえば、NVIDIA の RTX 2080 Ti には 352 ビット幅のバスを搭載しています。これにより、より多くの情報を同時に転送できます。次に、GPU は特定の高性能メモリを使用します。NVIDIA の RTX や Titan シリーズなどのコンシューマグレードのデバイスは、通常、総帯域幅が 500 GB/ 秒を超える [GDDR6](https://en.wikipedia.org/wiki/GDDR6_SDRAM) チップを使用します。別の方法として、HBM (高帯域幅メモリ) モジュールを使用する方法があります。非常に異なるインターフェースを使用し、専用のシリコンウェーハ上のGPUと直接接続します。このため、それらは非常に高価になり、その使用は通常、NVIDIA Volta V100 シリーズのアクセラレータなどのハイエンドサーバーチップに限定されます。当然のことながら、GPU メモリは一般に CPU メモリよりもコストが高いため、GPU メモリは CPU メモリよりもはるかに小さくなります。私たちの目的のために、概して、それらのパフォーマンス特性は類似しており、はるかに高速です。この本の目的のために、詳細は無視しても問題ありません。これらは、GPU カーネルをチューニングして高スループットを実現する場合にのみ重要です。 

## ストレージ

RAMの重要な特徴のいくつかは*帯域幅*と*レイテンシ*であることがわかりました。ストレージデバイスについても同じことが言えますが、違いはさらに極端になる可能性があります。 

### ハードディスクドライブ

*ハードディスクドライブ* (HDD) は半世紀以上にわたって使用されてきました。簡単に言うと、任意のトラックで読み取りまたは書き込みできるように配置できるヘッド付きのスピニングプラッターが多数含まれています。ハイエンドディスクは、9 台のプラッタで最大 16 TB を格納できます。HDD の主な利点の 1 つは、比較的安価であることです。多くの欠点の 1 つは、典型的な致命的な障害モードと、読み取りレイテンシが比較的高いことです。

後者を理解するために、HDD が約 7,200 RPM (1 分あたりの回転数) で回転するという事実を考えてみましょう。それらがはるかに速ければ、プラッターに作用する遠心力のために粉砕されます。ディスク上の特定のセクタにアクセスする場合、これには大きな欠点があります。ディスクプラッタが所定の位置に回転するまで待つ必要があります (ヘッドを動かすことはできますが、実際のディスクは高速化できません)。したがって、要求されたデータが利用可能になるまで 8 ミリ秒以上かかることがあります。一般的な言い方をすると、HDD は約 100 IOPS (1 秒あたりの入出力動作) で動作します。この数字は、過去20年間本質的に変わっていません。さらに悪いことに、帯域幅を増やすことも同様に困難です（100～200 MB/秒程度）。結局のところ、各ヘッドはビットのトラックを読み取るため、ビットレートは情報密度の平方根にのみ比例します。その結果、HDD は急速にアーカイブストレージと非常に大きなデータセット用の低グレードストレージに追いやられてきています。 

### ソリッドステートドライブ

ソリッドステートドライブ (SSD) は、フラッシュメモリを使用して情報を永続的に保存します。これにより、保存されたレコードに「はるかに高速に」アクセスできるようになります。最新の SSD は 100,000 ～ 500,000 IOPS で動作します。つまり、HDD よりも最大 3 桁高速です。さらに、その帯域幅は1～3 Gb/sに達します。つまり、HDDよりも1桁高速です。これらの改善は、本当であるにはほとんど良すぎるように聞こえます。実際、SSD の設計方法により、次の注意点があります。 

* SSD は情報をブロック (256 KB 以上) 単位で格納します。それらは全体としてしか書けず、かなりの時間がかかります。そのため、SSD へのビット単位のランダム書き込みのパフォーマンスは非常に低くなります。同様に、ブロックを読み取り、消去してから新しい情報で書き直す必要があるため、一般にデータの書き込みにはかなりの時間がかかります。SSDコントローラとファームウェアは、これを軽減するアルゴリズムを開発しました。ただし、特に QLC (クアッドレベルセル) SSD では、書き込みが大幅に遅くなる可能性があります。パフォーマンス向上の鍵は、オペレーションの*キュー*を維持し、読み込みを優先し、可能であれば大きなブロックで書き込みを行うことです。
* SSD のメモリセルは比較的早く消耗します (多くの場合、すでに数千回の書き込みの後)。摩耗レベルの保護アルゴリズムは、劣化を多数のセルに分散させることができます。ただし、SSD をファイルのスワップや大量のログファイルの集約に使用することは推奨されません。
* 最後に、帯域幅の大幅な増加により、コンピューター設計者はSSDをPCIeバスに直接接続せざるを得なくなりました。これを処理できるドライブは NVMe (不揮発性メモリ拡張) と呼ばれ、最大 4 つの PCIe レーンを使用できます。これは PCIe 4.0 では最大 8 Gb/秒に相当します。

### クラウドストレージ

クラウドストレージは、設定可能な範囲のパフォーマンスを提供します。つまり、仮想マシンへのストレージの割り当ては、ユーザーが選択した量と速度の両方の点で動的です。小さなレコードが多いトレーニング中など、レイテンシーが高すぎる場合は常に、プロビジョニングされた IOPS 数を増やすことをお勧めします。 

## CPU

中央処理装置 (CPU) は、あらゆるコンピュータの中心的存在です。これらは、マシンコードを実行できる*プロセッサコア*、それらを接続する*バス* (特定のトポロジはプロセッサモデル、世代、ベンダーによって大きく異なります)、*キャッシュ*など、多くの主要コンポーネントで構成され、従来よりも広い帯域幅と低レイテンシーのメモリアクセスを可能にする*キャッシュ*メインメモリからの読み込みで可能です。最後に、最近の CPU のほとんどすべてに、メディア処理や機械学習で一般的な、高性能の線形代数と畳み込みを支援する*ベクトル処理ユニット* が搭載されています。 

![Intel Skylake consumer quad-core CPU.](../img/skylake.svg)
:label:`fig_skylake`

:numref:`fig_skylake` は、インテル Skylake コンシューマーグレードのクアッドコア CPU を示しています。GPU、キャッシュ、4 つのコアを接続するリングバスが統合されています。イーサネット、WiFi、Bluetooth、SSD コントローラ、USB などの周辺機器は、チップセットの一部であるか、CPU に直接接続 (PCIe) されています。 

### マイクロアーキテクチャ

各プロセッサコアは、かなり洗練されたコンポーネントセットで構成されています。詳細は世代やベンダーによって異なりますが、基本的な機能はほぼ標準的です。フロントエンドは命令をロードし、どの経路を取るかを予測しようとします (制御フローなど)。その後、命令はアセンブリコードからマイクロ命令にデコードされます。アセンブリコードは、プロセッサが実行する最下位レベルのコードではないことがよくあります。その代わり、複雑な命令は、より低レベルの演算のセットにデコードされることがあります。その後、実際の実行コアによって処理されます。多くの場合、後者は多くの操作を同時に実行できます。たとえば、:numref:`fig_cortexa77` の ARM Cortex A77 コアは、最大 8 つのオペレーションを同時に実行できます。 

![ARM Cortex A77 Microarchitecture.](../img/a77.svg)
:label:`fig_cortexa77`

つまり、効率の良いプログラムは、独立して実行できれば、クロックサイクルごとに複数の命令を実行できる可能性があります。すべてのユニットが同じように作られているわけではありません。整数命令に特化したものもあれば、浮動小数点のパフォーマンスに最適化されているものもあります。スループットを向上させるために、プロセッサは分岐命令で複数のコードパスを同時にたどり、分岐されなかった結果を破棄することもあります。これが分岐予測ユニットが (フロントエンドで) 重要であり、最も有望なパスのみが追求される理由です。 

### ベクトル化

ディープラーニングは、計算を大量に消費します。そのため、CPUを機械学習に適したものにするためには、1クロックサイクルで多くの演算を実行する必要があります。これはベクトル単位で実現されます。それらの名前は異なります: on ARM they are called NEON, on x86 they (a recent generation) are referred to as [AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) units. A common aspect is that they are able to perform SIMD (single instruction multiple data) operations. :numref:`fig_neon128` は、ARM で 1 クロックサイクルで 8 個の短い整数を追加する方法を示しています。 

![128 bit NEON vectorization.](../img/neon128.svg)
:label:`fig_neon128`

アーキテクチャの選択にもよりますが、このようなレジスタは最大 512 ビット長で、最大 64 組の数値の組み合わせが可能です。たとえば、2 つの数値を掛けて 3 番目の数に加算する場合がありますが、これは融合乗算加算とも呼ばれます。インテルの [OpenVino](https://01.org/openvinotoolkit) は、サーバーグレードの CPU でのディープラーニングで相当なスループットを実現するために、これらを使用しています。ただし、この数値は GPU が達成できる能力によって完全に小さくなっていることに注意してください。たとえば、NVIDIA の RTX 2080 Ti には 4,352 個の CUDA コアがあり、各コアはいつでもこのような操作を処理できます。 

### キャッシュ

2 GHz の周波数で動作する、上記の次の状況 : we have a modest CPU core with 4 cores as depicted in :numref:`fig_skylake` について考えてみます。さらに、IPC (クロックあたりの命令数) カウントが 1 で、ユニットで 256 ビット幅の AVX2 が有効になっていると仮定します。さらに、AVX2 の操作に使用されるレジスタのうち、少なくとも 1 つをメモリから取り出す必要があると仮定します。これは、CPU がクロックサイクルあたり $4 \times 256 \text{ bit} = 128 \text{ bytes}$ のデータを消費することを意味します。毎秒$2 \times 10^9 \times 128 = 256 \times 10^9$バイトをプロセッサに転送できない限り、処理要素は枯渇してしまいます。残念ながら、このようなチップのメモリインタフェースは、20～40 Gb/秒のデータ転送しかサポートしていません。つまり、1桁少ないデータ転送しかサポートしていません。この修正は、*new* データを可能な限りメモリから読み込まないようにし、それを CPU にローカルにキャッシュするためです。ここでキャッシュが役に立ちます。通常、次の名前または概念が使用されます。 

* **レジスタ**は厳密に言うとキャッシュの一部ではありません。彼らはステージの指示を助けます。つまり、CPUレジスタは、CPUが遅延ペナルティなしでクロックスピードでアクセスできるメモリ位置です。CPU には数十個のレジスタがあります。レジスタを効率的に使用するかどうかは、コンパイラ (またはプログラマ) に任されています。たとえば、C プログラミング言語には `register` というキーワードがあります。
* **L1 キャッシュ** は、高いメモリ帯域幅要件に対する防御の最前線です。L1 キャッシュは小さく (一般的なサイズは 32 ～ 64 KB)、データキャッシュと命令キャッシュに分割されることがよくあります。L1 キャッシュにデータが見つかると、アクセスが非常に高速になります。そこに見つからない場合、検索はキャッシュ階層の下方へ進みます。
* **L2キャッシュ**が次の停留所です。アーキテクチャ設計とプロセッササイズによっては、これらが排他的になる場合があります。特定のコアからしかアクセスできない場合もあれば、複数のコア間で共有される場合もあります。L2 キャッシュは L1 よりも大きく (通常、コアあたり 256 ～ 512 KB)、低速です。さらに、L2 内の何かにアクセスするには、まず、データが L1 にないことを確認する必要があります。これにより、レイテンシーが少し増えます。
* **L3 キャッシュ** は複数のコアで共有され、非常に大きくなる可能性があります。AMDのEpyc 3サーバーCPUには、256 MBのキャッシュが複数のチップレットに分散されています。より一般的な数値は 4 ～ 8 MB の範囲です。

次に必要となるメモリ素子を予測することは、チップ設計における重要な最適化パラメータの 1 つです。たとえば、ほとんどのキャッシュアルゴリズムは逆方向ではなく*先読み*を試みるため、メモリを*forward* 方向にトラバースすることをお勧めします。同様に、メモリアクセスパターンをローカルにしておくことは、パフォーマンスを向上させる良い方法です。 

キャッシュの追加は両刃の剣です。一方では、プロセッサコアがデータを枯渇させないようにします。同時に、チップサイズが大きくなり、処理能力の向上に費やされていた領域が消費されます。さらに、*キャッシュミス*はコストがかかる可能性があります。:numref:`fig_falsesharing` に示すように、最悪のシナリオ、*誤った共有* を考えてみましょう。プロセッサ 1 のスレッドがデータを要求すると、メモリ位置はプロセッサ 0 にキャッシュされます。これを取得するには、プロセッサ 0 は実行中の処理を停止し、情報をメインメモリに書き戻し、プロセッサ 1 にメモリから読み取らせる必要があります。この操作中、両方のプロセッサが待機します。このようなコードは、効率的なシングルプロセッサ実装と比較して、複数のプロセッサで*より遅く*実行される可能性があります。これが、(物理サイズ以外に) キャッシュサイズに実質的な制限がある理由の 1 つです。 

![False sharing (image courtesy of Intel).](../img/falsesharing.svg)
:label:`fig_falsesharing`

## GPU とその他のアクセラレータ

GPU がなければディープラーニングは成功しなかったと言っても過言ではありません。同様に、ディープラーニングによってGPUメーカーの運勢が大幅に増加したと主張するのはかなり合理的です。このハードウェアとアルゴリズムの共進化により、ディープラーニングの良し悪しを問わず、統計モデリングのパラダイムが望ましい状況になりました。したがって、GPU と TPU :cite:`Jouppi.Young.Patil.ea.2017` などの関連アクセラレータがもたらす具体的な利点を理解しておく必要があります。 

注目すべき点は、実際によく見られる区別です。アクセラレータはトレーニングまたは推論用に最適化されています。後者の場合は、ネットワーク内の前方伝播を計算するだけで済みます。バックプロパゲーションのために中間データを保存する必要はありません。さらに、非常に正確な計算は必要ないかもしれません (通常、FP16 または INT8 で十分です)。一方、トレーニング中は、すべての中間結果に勾配を計算するためのストレージが必要です。さらに、勾配を累積するには、数値アンダーフロー (またはオーバーフロー) を回避するために、より高い精度が必要です。これは、FP16 (または FP32 との混合精度) が最小要件であることを意味します。これらすべてにより、より高速で大容量のメモリ (HBM2 対 GDDR6) とより高い処理能力が必要となります。たとえば、NVIDIA の [Turing](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/) T4 GPU は推論用に最適化されていますが、V100 GPU はトレーニングには適しています。 

:numref:`fig_neon128` に示されているベクトル化を思い出してください。プロセッサコアにベクトルユニットを追加することで、スループットを大幅に向上させることができました。たとえば、:numref:`fig_neon128` の例では、16 個の操作を同時に実行できました。まず、ベクトル間の演算だけでなく、行列間の演算も最適化した演算を追加するとどうなるでしょうか。この戦略により、テンソルコアが生まれました (まもなく説明します)。次に、さらに多くのコアを追加したらどうなるでしょうか。簡単に言うと、これらの 2 つの戦略は GPU での設計上の決定をまとめたものです。:numref:`fig_turing_processing_block` は、基本的な処理ブロックの概要を示しています。16 個の整数と 16 個の浮動小数点単位が含まれます。さらに、2 つのテンソルコアにより、ディープラーニングに関連する追加演算の狭いサブセットが高速化されます。各ストリーミングマルチプロセッサは、4 つのブロックで構成されます。 

![NVIDIA Turing processing block (image courtesy of NVIDIA).](../img/turing-processing-block.png)
:width:`150px`
:label:`fig_turing_processing_block`

次に、12 個のストリーミングマルチプロセッサが、ハイエンドの TU102 プロセッサを構成するグラフィックス処理クラスタにグループ化されます。十分なメモリチャネルと L2 キャッシュがセットアップを補完します。:numref:`fig_turing` には関連する詳細があります。このようなデバイスを設計する理由の 1 つは、チップの小型化や歩留まりの問題 (障害のあるモジュールがアクティブ化されない可能性がある) に対処するために、必要に応じて個々のブロックを追加または削除できることです。幸いなことに、このようなデバイスのプログラミングは、CUDAとフレームワークコードのレイヤーの下にあるカジュアルなディープラーニング研究者からは十分に隠されています。特に、使用可能なリソースがあれば、複数のプログラムが GPU 上で同時に実行される可能性があります。それでも、デバイスのメモリに収まらないモデルを選択しないように、デバイスの制限を認識しておく必要があります。 

![NVIDIA Turing architecture (image courtesy of NVIDIA)](../img/turing.png)
:width:`350px`
:label:`fig_turing`

最後に詳しく説明する価値があるのは、*テンソルコア*です。これらは、特にディープラーニングに効果的な最適化された回路を追加するという最近の傾向の一例です。たとえば、TPU は高速行列乗算のためにシストリック配列 :cite:`Kung.1988` を追加しました。この設計では、ごく少数 (TPU の第 1 世代) の大規模なオペレーションをサポートするように設計されました。テンソルコアは反対側にあります。数値精度に応じて $4 \times 4$ ～ $16 \times 16$ の行列を含む小規模な演算に最適化されています。:numref:`fig_tensorcore` に、最適化の概要を示します。 

![NVIDIA tensor cores in Turing (image courtesy of NVIDIA).](../img/tensorcore.jpg)
:width:`400px`
:label:`fig_tensorcore`

明らかに、計算のために最適化すると、ある程度の妥協をすることになります。その1つは、GPUが割り込みやスパースデータの処理にあまり適していないことです。[Gunrock](https://github.com/gunrock/gunrock) :cite:`Wang.Davidson.Pan.ea.2016`) のように注目すべき例外はありますが、スパース行列とベクトルのアクセスパターンは、GPU が優れている高帯域幅のバーストリード操作ではうまくいきません。両方の目標を一致させることは活発な研究分野です。たとえば、グラフのディープラーニング用に調整されたライブラリ [DGL](http://dgl.ai) を参照してください。 

## ネットワークとバス

単一のデバイスでは最適化が不十分である場合は、そのデバイスとの間でデータを転送して処理を同期させる必要があります。ここでネットワークやバスが役に立ちます。帯域幅、コスト、距離、柔軟性など、多くの設計パラメータがあります。一方では、範囲がかなり良く、非常に使いやすく（結局配線なし）、安価ですが、比較的平凡な帯域幅と遅延を提供するWiFiがあります。正しい考えを持つ機械学習の研究者は、これを使ってサーバーのクラスターを構築することはないでしょう。以下では、ディープラーニングに適したインターコネクトに注目します。 

* **PCIe** は、レーンあたり非常に高帯域幅のポイントツーポイント接続 (PCIe 4.0 で 16 レーンスロットで最大 32 Gb/s) 用の専用バスです。レイテンシーは 1 桁マイクロ秒 (5 μs) 程度です。PCIe リンクは貴重です。プロセッサーの数は限られています。AMDのEPYC 3は128レーン、インテルのXeonはチップあたり最大48レーン、デスクトップグレードのCPUではそれぞれ20（Ryzen 9）と16（Core i9）です。GPU には通常 16 レーンがあるため、これにより CPU に全帯域幅で接続できる GPU の数が制限されます。結局のところ、ストレージやイーサネットなどの他の高帯域幅周辺機器とリンクを共有する必要があります。RAM アクセスと同様に、パケットのオーバーヘッドが減少するため、大量の一括転送が適しています。
* **Ethernet** は、コンピュータを接続する最も一般的な方法です。PCIeよりも大幅に低速ですが、設置が非常に安価で弾力性があり、はるかに長い距離をカバーできます。低グレードのサーバーの一般的な帯域幅は 1 Gbit/s です。ハイエンドデバイス (クラウドの [C5 instances](https://aws.amazon.com/ec2/instance-types/c5/) など) は、10 ～ 100 Gbit/s の帯域幅を提供します。これまでのすべてのケースと同様に、データ伝送にはかなりのオーバーヘッドがあります。生のイーサネットを直接使用することはほとんどなく、物理インターコネクト (UDP や TCP/IP など) の上で実行されるプロトコルを使用することに注意してください。これにより、さらにオーバーヘッドが追加されます。PCIe と同様、イーサネットは、コンピューターとスイッチなどの 2 つのデバイスを接続するように設計されています。
* **スイッチ** を使用すると、任意のペアで (通常は全帯域幅) ポイントツーポイント接続を同時に実行できる方法で、複数のデバイスを接続できます。たとえば、イーサネットスイッチは 40 台のサーバを高い断面帯域幅で接続する場合があります。スイッチは従来のコンピュータネットワークに固有のものではないことに注意してください。PCIe レーンでも [switched](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches) になることがあります。これは、[P2 instances](https://aws.amazon.com/ec2/instance-types/p2/) の場合のように、多数の GPU をホストプロセッサに接続する場合などに発生します。
* **nvLink** は、非常に高帯域幅の相互接続において PCIe の代替手段となります。リンクあたり最大 300 Gbit/s のデータ転送速度を提供します。サーバー GPU (Volta V100) には 6 つのリンクがありますが、コンシューマーグレードの GPU (RTX 2080 Ti) にはリンクが 1 つしかなく、100 ギガビット/秒の低速で動作します。GPU 間で高いデータ転送を実現するには、[NCCL](https://github.com/NVIDIA/nccl) を使用することをお勧めします。

## レイテンシーの数値が増える

:numref:`table_latency_numbers` と :numref:`table_latency_numbers_tesla` のサマリーは [Eliot Eshelman](https://gist.github.com/eshelman) からのもので、番号の更新バージョンを [GitHub gist](https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646) として管理しています。 

:共通のレイテンシー番号。 

| Action | Time | Notes |
| :----------------------------------------- | -----: | :---------------------------------------------- |
| L1 cache reference/hit                     | 1.5 ns | 4 cycles                                        |
| Floating-point add/mult/FMA                | 1.5 ns | 4 cycles                                        |
| L2 cache reference/hit                     |   5 ns | 12 ~ 17 cycles                                  |
| Branch mispredict                          |   6 ns | 15 ~ 20 cycles                                  |
| L3 cache hit (unshared cache)              |  16 ns | 42 cycles                                       |
| L3 cache hit (shared in another core)      |  25 ns | 65 cycles                                       |
| Mutex lock/unlock                          |  25 ns |                                                 |
| L3 cache hit (modified in another core)    |  29 ns | 75 cycles                                       |
| L3 cache hit (on a remote CPU socket)      |  40 ns | 100 ~ 300 cycles (40 ~ 116 ns)                  |
| QPI hop to a another CPU (per hop)         |  40 ns |                                                 |
| 64MB memory ref. (local CPU)          |  46 ns | TinyMemBench on Broadwell E5-2690v4             |
| 64MB memory ref. (remote CPU)         |  70 ns | TinyMemBench on Broadwell E5-2690v4             |
| 256MB memory ref. (local CPU)         |  75 ns | TinyMemBench on Broadwell E5-2690v4             |
| Intel Optane random write                  |  94 ns | UCSD Non-Volatile Systems Lab                   |
| 256MB memory ref. (remote CPU)        | 120 ns | TinyMemBench on Broadwell E5-2690v4             |
| Intel Optane random read                   | 305 ns | UCSD Non-Volatile Systems Lab                   |
| Send 4KB over 100 Gbps HPC fabric          |   1 μs | MVAPICH2 over Intel Omni-Path                   |
| Compress 1KB with Google Snappy            |   3 μs |                                                 |
| Send 4KB over 10 Gbps ethernet             |  10 μs |                                                 |
| Write 4KB randomly to NVMe SSD             |  30 μs | DC P3608 NVMe SSD (QOS 99% is 500μs)            |
| Transfer 1MB to/from NVLink GPU            |  30 μs | ~33GB/s on NVIDIA 40GB NVLink                 |
| Transfer 1MB to/from PCI-E GPU             |  80 μs | ~12GB/s on PCIe 3.0 x16 link                  |
| Read 4KB randomly from NVMe SSD            | 120 μs | DC P3608 NVMe SSD (QOS 99%)                     |
| Read 1MB sequentially from NVMe SSD        | 208 μs | ~4.8GB/s DC P3608 NVMe SSD                    |
| Write 4KB randomly to SATA SSD             | 500 μs | DC S3510 SATA SSD (QOS 99.9%)                   |
| Read 4KB randomly from SATA SSD            | 500 μs | DC S3510 SATA SSD (QOS 99.9%)                   |
| Round trip within same datacenter          | 500 μs | One-way ping is ~250μs                          |
| Read 1MB sequentially from SATA SSD        |   2 ms | ~550MB/s DC S3510 SATA SSD                    |
| Read 1MB sequentially from disk            |   5 ms | ~200MB/s server HDD                           |
| Random Disk Access (seek+rotation)         |  10 ms |                                                 |
| Send packet CA->Netherlands->CA            | 150 ms |                                                 |
:label:`table_latency_numbers`

:NVIDIA テスラ GPU のレイテンシー番号。 

| Action | Time | Notes |
| :------------------------------ | -----: | :---------------------------------------- |
| GPU Shared Memory access        |  30 ns | 30~90 cycles (bank conflicts add latency) |
| GPU Global Memory access        | 200 ns | 200~800 cycles                            |
| Launch CUDA kernel on GPU       |  10 μs | Host CPU instructs GPU to start kernel    |
| Transfer 1MB to/from NVLink GPU |  30 μs | ~33GB/s on NVIDIA 40GB NVLink           |
| Transfer 1MB to/from PCI-E GPU  |  80 μs | ~12GB/s on PCI-Express x16 link         |
:label:`table_latency_numbers_tesla`

## [概要

* デバイスにはオペレーションのオーバーヘッドがあります。したがって、小さな振替を多くするのではなく、少人数の大きな振替を目指すことが重要です。これは RAM、SSD、ネットワーク、GPU に適用されます。
* ベクタ変換はパフォーマンスにとって重要です。アクセラレーターの特定の能力を認識していることを確認してください。たとえば、一部のインテル Xeon CPU は INT8 オペレーションに特に適しています。NVIDIA ボルタ GPU は FP16 マトリックスマトリクス演算に優れており、NVIDIA チューリングは FP16、INT8、および INT4 オペレーションで優れています。
* データ型が小さいことによる数値オーバーフローは、トレーニング中 (推論中はそれほどではありませんが) 問題になることがあります。
* エイリアシングはパフォーマンスを著しく低下させる可能性があります。たとえば、64 ビット CPU でのメモリアライメントは 64 ビット境界を基準にして行う必要があります。GPU では、畳み込みのサイズをテンソルコアなどに合わせて調整しておくとよいでしょう。
* アルゴリズムをハードウェア (メモリフットプリント、帯域幅など) に合わせます。パラメーターをキャッシュにフィッティングすると、大幅な高速化 (桁違い) を実現できます。
* 実験結果を検証する前に、新しいアルゴリズムの性能を紙にスケッチすることをお勧めします。桁違い以上の不一致が懸念される理由です。
* プロファイラーを使用して、パフォーマンスのボトルネックをデバッグします。
* トレーニングハードウェアと推論ハードウェアには、価格とパフォーマンスの点で異なるスイートスポットがあります。

## 演習

1. C コードを記述して、外部メモリー・インターフェースに対してメモリ・アラインメントとミスアライメントされたメモリーへのアクセス速度に違いがないかどうかをテストします。ヒント:キャッシュ効果には注意が必要です。
1. メモリに順次アクセスするか、指定したストライドでアクセスする場合の速度の違いを検定します。
1. CPU のキャッシュサイズをどのように測定できますか。
1. 帯域幅を最大化するために、複数のメモリチャネルにわたってデータをどのようにレイアウトしますか？小さな糸が多い場合はどうレイアウトしますか？
1. エンタープライズクラスの HDD は 10,000 rpm で回転しています。HDDがデータを読み取るまでに最悪の場合に費やす必要のある絶対最小時間はどれくらいですか（ヘッドがほぼ瞬時に動くと想定できます）。2.5インチHDDが商用サーバー (3.5インチおよび5.25インチドライブに比べて) で人気が高まっているのはなぜですか?
1. HDD メーカーがストレージ密度を 1 Tbit /平方インチから 5 Tbit /平方インチに増やしたと仮定します。2.5インチHDDのリングにはどのくらいの量の情報を保存できますか？インナートラックとアウタートラックに違いはありますか？
1. データタイプが 8 ビットから 16 ビットになると、シリコンの量が約 4 倍に増加します。なぜ？NVIDIA が Turing GPU に INT4 オペレーションを追加したのはなぜですか？
1. メモリを前方に読み取るのと逆方向に読むのはどれくらい速いですか？この数値は、コンピューターや CPU ベンダーによって異なりますか。なぜ？C コードを書いて試してみてください。
1. ディスクのキャッシュサイズを測定できますか。一般的なHDDにはどのようなものがありますか？SSD にはキャッシュが必要ですか？
1. イーサネット経由でメッセージを送信する場合のパケットオーバーヘッドを測定します。UDP 接続と TCP/IP 接続の違いを調べます。
1. ダイレクトメモリアクセスでは、CPU 以外のデバイスがメモリに直接書き込み (読み取り) できます。なぜこれが良いアイデアなのですか？
1. Turing T4 GPU のパフォーマンス数値を見てください。FP16からINT8、INT4に移行すると、パフォーマンスが「2倍だけ」になるのはなぜですか？
1. サンフランシスコとアムステルダム間の往復でパケットを取るのにかかる最短時間はどれくらいですか？ヒント:距離は 10,000 km であると仮定できます。

[Discussions](https://discuss.d2l.ai/t/363)
