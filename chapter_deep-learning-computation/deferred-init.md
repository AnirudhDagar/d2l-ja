# 遅延初期化
:label:`sec_deferred_init`

これまでのところ、ネットワークの設定がだらしないことで逃げたように見えるかもしれません。具体的には、次のような直感的でないことを行いましたが、動作するようには思えないかもしれません。 

* 入力次元を指定せずにネットワークアーキテクチャを定義しました。
* 前のレイヤーの出力次元を指定せずにレイヤーを追加しました。
* モデルに含めるべきパラメータの数を決定するのに十分な情報を提供する前に、これらのパラメータを「初期化」しました。

私たちのコードがまったく実行されていることに驚くかもしれません。結局のところ、ディープラーニングフレームワークがネットワークの入力次元を判断する方法はありません。ここでの秘訣は、フレームワークが初期化を*延期*し、最初にデータをモデルに渡すまで待って、各レイヤーのサイズをその場で推測することです。 

その後、畳み込みニューラルネットワークで作業する場合、入力次元 (画像の解像度) が後続の各層の次元性に影響を与えるため、この手法はさらに便利になります。したがって、コードの記述時に次元が何であるかを知る必要なくパラメータを設定できるため、モデルを指定して後で変更するタスクが大幅に簡略化されます。次に、初期化の仕組みについて詳しく説明します。 

## ネットワークのインスタンス化

はじめに、MLP をインスタンス化してみましょう。

```{.python .input}
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()

def get_net():
    net = nn.Sequential()
    net.add(nn.Dense(256, activation='relu'))
    net.add(nn.Dense(10))
    return net

net = get_net()
```

```{.python .input}
#@tab tensorflow
import tensorflow as tf

net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10),
])
```

この時点では、入力ディメンションが不明のままであるため、ネットワークは入力レイヤのウェイトのディメンションを認識できない可能性があります。そのため、フレームワークはまだパラメータを初期化していません。以下のパラメータにアクセスして確認します。

```{.python .input}
print(net.collect_params)
print(net.collect_params())
```

```{.python .input}
#@tab tensorflow
[net.layers[i].get_weights() for i in range(len(net.layers))]
```

:begin_tab:`mxnet`
パラメーターオブジェクトが存在する間は、各レイヤーへの入力次元は -1 としてリストされることに注意してください。MXNet は、パラメーターの次元が不明のままであることを示すために、特別な値 -1 を使用します。この時点で `net[0].weight.data()` にアクセスしようとすると、パラメータにアクセスする前にネットワークを初期化する必要があることを示すランタイムエラーが発生します。ここで、`initialize` 関数でパラメーターを初期化しようとするとどうなるか見てみましょう。
:end_tab:

:begin_tab:`tensorflow`
各レイヤオブジェクトは存在しますが、ウェイトは空です。`net.get_weights()` を使用すると、ウェイトがまだ初期化されていないため、エラーがスローされます。
:end_tab:

```{.python .input}
net.initialize()
net.collect_params()
```

:begin_tab:`mxnet`
ご覧のとおり、何も変わっていません。入力次元が不明な場合、initialize を呼び出してもパラメーターは正しく初期化されません。代わりに、この呼び出しは MXNet に登録します。MXNet は、パラメーターの初期化を希望する (オプションで、どのディストリビューションに応じて)。
:end_tab:

次に、ネットワークを介してデータを渡し、フレームワークが最終的にパラメータを初期化するようにします。

```{.python .input}
X = np.random.uniform(size=(2, 20))
net(X)

net.collect_params()
```

```{.python .input}
#@tab tensorflow
X = tf.random.uniform((2, 20))
net(X)
[w.shape for w in net.get_weights()]
```

入力の次元 20 がわかるとすぐに、フレームワークは値 20 を差し込むことで第 1 層の重み行列の形状を識別できます。最初のレイヤーの形状を認識すると、フレームワークは2番目のレイヤーに進み、すべての形状がわかるまで計算グラフを通じて続きます。この場合、遅延初期化が必要なのは第1層のみですが、フレームワークは順次初期化されます。すべてのパラメーターの形状がわかれば、フレームワークは最終的にパラメーターを初期化できます。 

## [概要

* 遅延初期化は便利で、フレームワークがパラメーターの形状を自動的に推測できるため、アーキテクチャーの変更が容易になり、一般的なエラーの原因を 1 つ排除できます。
* モデルを介してデータを渡し、フレームワークが最終的にパラメーターを初期化するようにできます。

## 演習

1. 入力次元を最初のレイヤーに指定し、後続のレイヤーには指定しないとどうなりますか？すぐに初期化できますか？
1. 不一致の寸法を指定した場合はどうなりますか。
1. さまざまな次元の入力があるとしたら、何をする必要がありますか？ヒント:パラメータ同点を見てください。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/280)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/281)
:end_tab:
