# マルチスケールオブジェクト検出
:label:`sec_multiscale-object-detection`

:numref:`sec_anchor` では、入力イメージの各ピクセルを中心に複数のアンカーボックスを生成しました。基本的に、これらのアンカーボックスはイメージのさまざまな領域のサンプルを表します。ただし、*ピクセルごとに生成されるアンカーボックスが多すぎて計算できない場合があります。$561 \times 728$ の入力イメージについて考えてみてください。各ピクセルを中心として形状が変化するアンカーボックスが 5 つ生成された場合、200 万個を超えるアンカーボックス ($561 \times 728 \times 5$) をイメージ上でラベル付けして予測する必要があります。 

## マルチスケールアンカーボックス
:label:`subsec_multiscale-anchor-boxes`

画像のアンカーボックスを減らすのは難しくないことに気付いたかもしれません。たとえば、入力イメージからピクセルのごく一部を均一にサンプリングして、その中心にアンカーボックスを生成することができます。さらに、さまざまなスケールで、サイズの異なる数のアンカーボックスを生成できます。直感的には、小さいオブジェクトは大きいオブジェクトよりもイメージ上に表示される可能性が高くなります。たとえば、$1 \times 1$、$1 \times 2$、$2 \times 2$ のオブジェクトは $2 \times 2$ イメージにそれぞれ 4 つ、2 つ、1 つの方法で表示できます。したがって、小さいアンカーボックスを使用して小さなオブジェクトを検出すると、より多くの領域をサンプリングでき、大きいオブジェクトではサンプリングするリージョンを減らすことができます。 

複数の縮尺でアンカーボックスを生成する方法を示すために、イメージを読みます。高さと幅はそれぞれ 561 ピクセルと 728 ピクセルです。

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import image, np, npx

npx.set_np()

img = image.imread('../img/catdog.jpg')
h, w = img.shape[:2]
h, w
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

img = d2l.plt.imread('../img/catdog.jpg')
h, w = img.shape[:2]
h, w
```

:numref:`sec_conv_layer` では、畳み込み層の 2 次元配列出力を特徴マップと呼んでいることを思い出してください。特徴マップの形状を定義することで、任意の画像上で均一にサンプリングされたアンカーボックスの中心を特定できます。 

`display_anchors` 関数は以下のように定義されています。[**各単位 (ピクセル) をアンカーボックスの中心として、フィーチャマップ (`fmap`) にアンカーボックス (`anchors`) を生成します。**] アンカーボックス (`anchors`) の $(x, y)$ 軸の座標値は、フィーチャマップ (`fmap`) の幅と高さで除算されるため、これらの値は 0 から 1 の間になります。フィーチャマップ内のアンカーボックスの相対位置を示します。 

アンカーボックス (`anchors`) の中心 (`anchors`) はフィーチャマップ (`fmap`) 上のすべてのユニットに分散されるため、これらの中心は入力イメージ上で相対的な空間位置で*均一*分布する必要があります。より具体的には、フィーチャマップ `fmap_w` と `fmap_h` の幅と高さがそれぞれ与えられた場合、次の関数は入力イメージの `fmap_h` 行と `fmap_w` 列のピクセルを「一様に」サンプリングします。均一にサンプリングされたピクセルを中心に、スケール `s` (リスト `s` の長さを 1 と仮定) と異なるアスペクト比 (`ratios`) のアンカーボックスが生成されます。

```{.python .input}
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # Values on the first two dimensions do not affect the output
    fmap = np.zeros((1, 10, fmap_h, fmap_w))
    anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = np.array((w, h, w, h))
    d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,
                    anchors[0] * bbox_scale)
```

```{.python .input}
#@tab pytorch
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # Values on the first two dimensions do not affect the output
    fmap = d2l.zeros((1, 10, fmap_h, fmap_w))
    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = d2l.tensor((w, h, w, h))
    d2l.show_bboxes(d2l.plt.imshow(img).axes,
                    anchors[0] * bbox_scale)
```

まず、[**小さな物体の検出について検討する**]。表示時に区別しやすくするために、中心が異なるアンカーボックスは重ならないようにしています。アンカーボックスの縮尺は 0.15、フィーチャマップの高さと幅は 4 に設定されています。画像上の 4 行 4 列のアンカーボックスの中心が均一に分布していることがわかります。

```{.python .input}
#@tab all
display_anchors(fmap_w=4, fmap_h=4, s=[0.15])
```

[**特徴マップの高さと幅を半分に減らし、より大きなアンカーボックスを使用して大きなオブジェクトを検出する**] に移ります。スケールを 0.4 に設定すると、いくつかのアンカーボックスが互いに重なり合ってしまいます。

```{.python .input}
#@tab all
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])
```

最後に、[**特徴マップの高さと幅をさらに半分に減らし、アンカーボックスの縮尺を 0.8 に増やします**]。これで、アンカーボックスの中心がイメージの中心になります。

```{.python .input}
#@tab all
display_anchors(fmap_w=1, fmap_h=1, s=[0.8])
```

## マルチスケール検出

マルチスケールのアンカーボックスを生成したので、さまざまなスケールでさまざまなサイズのオブジェクトを検出するために使用します。以下では、:numref:`sec_ssd` で実装する CNN ベースのマルチスケールオブジェクト検出方法を紹介します。 

ある尺度で、$h \times w$ という形状の $c$ フィーチャマップがあるとします。:numref:`subsec_multiscale-anchor-boxes` の方法を使用して、$hw$ セットのアンカーボックスを生成します。各セットには、中心が同じである $a$ 個のアンカーボックスがあります。たとえば、:numref:`subsec_multiscale-anchor-boxes` の実験の最初のスケールで、10 個の (チャネル数) の $4 \times 4$ フィーチャマップを指定すると、16 セットのアンカーボックスが生成され、各セットには同じ中心を持つ 3 つのアンカーボックスが含まれています。次に、各アンカーボックスに、グラウンドトゥルースバウンディングボックスに基づくクラスとオフセットのラベルが付けられます。現在のスケールでは、オブジェクト検出モデルは入力イメージ上の $hw$ セットのアンカーボックスのクラスとオフセットを予測する必要があります。 

ここで示す $c$ の特徴マップは、入力イメージに基づく CNN 順伝搬によって取得された中間出力であると仮定します。フィーチャマップごとに $hw$ 個の異なる空間位置が存在するため、同じ空間位置は $c$ 単位であると考えることができます。:numref:`sec_conv_layer` の受容野の定義によると、フィーチャマップの同じ空間位置にあるこれらの $c$ ユニットは、入力イメージ上で同じ受容場を持ち、同じ受容野の入力イメージ情報を表します。したがって、同じ空間位置にあるフィーチャマップの $c$ 単位を、この空間位置を使用して生成された $a$ アンカーボックスのクラスとオフセットに変換できます。基本的に、特定の受容野の入力画像の情報を使用して、入力画像上の受容野に近いアンカーボックスのクラスとオフセットを予測します。 

異なるレイヤーにある特徴マップが入力イメージ上に可変サイズの受容体をもつ場合、それらを使用してサイズの異なるオブジェクトを検出できます。たとえば、出力層に近い特徴マップの単位がより広い受容場を持つニューラルネットワークを設計して、入力イメージからより大きな物体を検出できるようにすることができます。 

簡単に言うと、ディープニューラルネットワークにより、マルチスケールの物体検出のために、複数レベルの画像のレイヤーワイズ表現を活用できます。:numref:`sec_ssd` の具体的な例を通して、これがどのように機能するかを示します。 

## [概要

* 複数の尺度で、異なるサイズのアンカーボックスを生成して、サイズの異なるオブジェクトを検出できます。
* 特徴マップの形状を定義することで、任意の画像上で均一にサンプリングされたアンカーボックスの中心を特定できます。
* 特定の受容野の入力画像の情報を使用して、入力画像上の受容野に近いアンカーボックスのクラスとオフセットを予測します。
* ディープラーニングにより、マルチスケールの物体検出のために、複数レベルの画像のレイヤーワイズ表現を活用できます。

## 演習

1. :numref:`sec_alexnet` での議論によると、ディープニューラルネットワークは画像の抽象化レベルを上げながら階層的な特徴を学習します。マルチスケールの物体検出において、異なる縮尺での特徴マップは異なる抽象化レベルに対応していますか？なぜ、なぜそうではないのですか？
1. :numref:`subsec_multiscale-anchor-boxes` の実験の最初のスケール (`fmap_w=4, fmap_h=4`) で、重なり合う可能性がある一様分布のアンカーボックスを生成します。
1. 形状が $1 \times c \times h \times w$ の特徴マップ変数があるとします。$c$、$h$、および $w$ はそれぞれ特徴マップの高さ、幅です。この変数をアンカーボックスのクラスとオフセットにどのように変換できますか？出力の形状はどのようなものですか？

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/371)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/1607)
:end_tab:
