# 注意メカニズム
:label:`chap_attention`

霊長類の視覚系の視神経は、脳が完全に処理できるものをはるかに超える大量の感覚入力を受けます。幸いなことに、すべての刺激が同じように作られるわけではありません。集中と意識の集中により、霊長類は複雑な視覚環境の中で、獲物や捕食者などの対象物に注意を向けることができました。ごく一部の情報だけに注目する能力は進化的に重要であり、人類は生きて成功することができます。 

科学者たちは、19世紀から認知神経科学の分野で注目を集めてきました。この章では、ビジュアルシーンにアテンションがどのように展開されるかを説明する、一般的なフレームワークを復習することから始めます。このフレームワークのアテンションキューにインスパイアされ、このようなアテンションキューを活用したモデルを設計します。特に、1964年のNadaraya-Wastonカーネル回帰は、*注意メカニズム*による機械学習の簡単なデモンストレーションです。 

次に、ディープラーニングにおけるアテンションモデルの設計に広く使われてきたアテンション関数について紹介します。具体的には、双方向に整列でき、微分可能な深層学習における画期的な注意モデルである*Bahdanau attention* を設計するために、これらの関数を使用する方法を示します。 

最終的には、より最近の
*マルチヘッドアテンション*
と*自己注意*の設計については、注意メカニズムのみに基づいた*トランスフォーマー*アーキテクチャについて説明します。2017年の提案以来、トランスフォーマーは言語、ビジョン、スピーチ、強化学習などの最新のディープラーニングアプリケーションに普及してきました。

```toc
:maxdepth: 2

attention-cues
nadaraya-watson
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
